{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basugautam/Reproducibility-Challenge-Project/blob/Architecture-Files/2_AUTOFORMER_revised_ERM_Exponential_Constant_losses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pPmd3KaH_a9B"
      },
      "outputs": [],
      "source": [
        "#  (a#) Why we are doing this:\n",
        "# , we aim to evaluate the effectiveness of different loss shaping strategies — namely **ERM (Empirical Risk Minimization)**, **Exponential**, and **Constant** — in training the Autoformer model.\n",
        "# We visualize the learning progress of the model using different types of penalties on the loss function. Just like comparing the weather conditions in three cities (Toronto, Vancouver, Calgary), we observe how each penalty affects the learning trajectory.\n",
        "# By comparing how each loss function strategy (ERM, Exponential, and Constant) affects training, we gain insight into which one results in the best learning for our time series forecasting problem.\n",
        "\n",
        "#  (b#) How this works:\n",
        "# - The training loop for **Autoformer** is established, where we will compute and apply the loss at each epoch.\n",
        "# - Different loss shaping strategies (ERM, Exponential, Constant) are applied to regularize the training process in different ways.\n",
        "# - We will visualize the loss for each strategy over the epochs to track how the model’s performance changes, like taking photos of the changing weather in each city at different times.\n",
        "# By using **color-coded graphs**, we can easily compare the performance of each strategy.\n",
        "# We will apply the **ERM (Sky Blue)** strategy as a baseline, **Exponential (Orange)** to highlight rapid changes, and **Constant (Green)** for stable performance across epochs.\n",
        "\n",
        "#  (c#) Explanation of terms:\n",
        "# - **ERM**: This is the basic, vanilla loss function (Mean Squared Error), which just compares the predictions to the ground truth, without any penalties. Think of this as an unseasoned dish — simple but effective.\n",
        "# - **Exponential**: This strategy introduces an exponentially growing penalty on the loss if the model deviates, like the consequences of breaking school rules that increase rapidly.\n",
        "# - **Constant**: This strategy applies a fixed penalty on the loss, irrespective of the model’s performance. It's like giving the same gentle nudge regardless of the situation.\n",
        "# - **Epoch**: An epoch is one complete cycle through the training data. Like attending a full day of classes in a city, we go through all the data once.\n",
        "# - **Loss**: Loss is the difference between the model’s prediction and the true value. It measures how far off we are, just like checking how far we’ve strayed from the expected route.\n",
        "# - **Color coding**: We use distinct colors (Sky Blue, Orange, Green) to distinguish between each strategy and make the plot visually clear.\n",
        "# - **Line styles**: Solid, dashed, and dash-dot lines are used to differentiate between the strategies on the plot.\n",
        "\n",
        "#  (d#) What we will achieve:\n",
        "# The end result of these operations will give us three graphs showing how each strategy affects training. By doing so, we’ll have clear insight into which shaping constraint (weather) gives the best results for our Autoformer (model).\n",
        "# This helps us choose the optimal regularization strategy for time-series forecasting, ensuring that our model can learn from the data efficiently.\n",
        "# By the end of the process, we will be able to select the best strategy based on its performance (loss behavior) over epochs.\n",
        "\n",
        "# (e#) Code for Visualization:\n",
        "# In this section, we will plot the loss trends for all three strategies — ERM, Exponential, and Constant.\n",
        "# This will help us visually compare their performances and see which one converges faster, or perhaps gives us more stable results.\n",
        "\n",
        "#  (f#) Loss Tracking for ERM, Exponential, and Constant:\n",
        "# The training loss for each strategy is tracked during the epochs. We will store these losses and plot them using **color-coded graphs** for better comparison.\n",
        "\n",
        "#  (g#) Visualizing the Results:\n",
        "# Using the matplotlib library, we will generate the loss curves for all three strategies. We’ll use distinct colors (Sky Blue, Orange, and Green) to differentiate them on the graph.\n",
        "# This visualization will show us how each loss function strategy behaves throughout the training process.\n",
        "\n",
        "#  (h#) Expected Outcome:\n",
        "# After running the training loop for each strategy, we will visualize the training loss trends. These graphs will allow us to clearly compare the performance of each strategy and make an informed decision about which regularization strategy yields the best model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "nmlqOAyN6bkU",
        "outputId": "185edc93-b2c7-4f1e-e242-981abf5122f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 502: Bad Gateway",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-13ab06886e25>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load ECL dataset from the UCI Machine Learning Repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mecl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Display the first few rows of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load ECL dataset from the UCI Machine Learning Repository\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv'\n",
        "ecl_data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "ecl_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idczbUcv_i5U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGboccNI6jMf"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in the ECL dataset\n",
        "ecl_missing = ecl_data.isnull().sum()\n",
        "\n",
        "# Fill missing values using forward fill method\n",
        "ecl_data_filled = ecl_data.fillna(method='ffill')\n",
        "\n",
        "# Display the first few rows after filling missing values\n",
        "ecl_data_filled.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FT1Lx1Z_mym"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W4v3AmD6rXv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler to normalize data\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize the 'T1' feature (temperature)\n",
        "ecl_data_filled['T1_normalized'] = scaler.fit_transform(ecl_data_filled[['T1']])\n",
        "\n",
        "# Display the first few rows of the normalized data\n",
        "ecl_data_filled[['T1', 'T1_normalized']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUIIFnX1_sRb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gK2gwKi60h5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the temperature data ('T1') to inspect its trend over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ecl_data_filled['T1'], label='Temperature (T1)')\n",
        "plt.title('Electricity Consumption Load - Temperature (T1)')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Temperature')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the normalized temperature data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ecl_data_filled['T1_normalized'], label='Normalized Temperature (T1)')\n",
        "plt.title('Normalized Temperature in ECL Dataset')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Normalized Temperature')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUFBreEX_3rg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzHDqAQ368EL"
      },
      "outputs": [],
      "source": [
        "# Display summary statistics of the preprocessed ECL data\n",
        "ecl_data_filled.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-92k_Tp_6Ec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHNoOGSc8yJW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of features excluding the 'date' column\n",
        "num_features = ecl_data_filled.shape[1] - 1  # Subtracting the 'date' column\n",
        "\n",
        "# Calculate the number of rows and columns for the subplot layout dynamically\n",
        "num_columns = 3  # Number of columns (we can adjust this as needed)\n",
        "num_rows = (num_features // num_columns) + (num_features % num_columns > 0)  # Calculate number of rows\n",
        "\n",
        "# Plot all features to inspect their behavior over time\n",
        "plt.figure(figsize=(14, num_rows * 3))  # Adjusting figure size based on the number of rows\n",
        "ecl_data_filled.drop(['date'], axis=1).plot(subplots=True, layout=(num_rows, num_columns), figsize=(14, num_rows * 3))\n",
        "plt.suptitle('All Features from ECL Dataset', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaOJ9_3RAkXx"
      },
      "outputs": [],
      "source": [
        "# Cell continuation from previous cell for inspecting data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of features excluding the 'date' column\n",
        "num_features = ecl_data_filled.shape[1] - 1  # Subtracting the 'date' column\n",
        "\n",
        "# Calculate the number of rows and columns for the subplot layout dynamically\n",
        "num_columns = 3  # Number of columns (you can adjust this as needed)\n",
        "num_rows = (num_features // num_columns) + (num_features % num_columns > 0)  # Calculate number of rows\n",
        "\n",
        "# Plot all features to inspect their behavior over time\n",
        "plt.figure(figsize=(14, num_rows * 3))  # Adjusting figure size based on the number of rows\n",
        "ecl_data_filled.drop(['date'], axis=1).plot(subplots=True, layout=(num_rows, num_columns), figsize=(14, num_rows * 3))\n",
        "plt.suptitle('All Features from ECL Dataset', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvJjGFbBAnQX"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   Visualizing all the features allows us to understand their individual behaviors and how they vary over time.\n",
        "#   This is critical for understanding potential trends, seasonality, and anomalies in the dataset before applying models.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   The code drops the 'date' column and plots each feature in a separate subplot to show how each one behaves over time.\n",
        "#   It uses dynamic calculations for the number of rows and columns to efficiently manage space and visual clarity.\n",
        "# c) Explanation of terms used:\n",
        "#   - **`ecl_data_filled`**: The dataset after filling missing values (through forward fill).\n",
        "#   - **`plot(subplots=True)`**: This method allows us to plot each column in the DataFrame as a subplot, making it easier to analyze multiple features.\n",
        "#   - **`layout=(num_rows, num_columns)`**: Specifies the layout of subplots to display them neatly in a grid format.\n",
        "#   - **`tight_layout()`**: Adjusts the spacing between subplots to prevent overlap.\n",
        "# d) What we will achieve from this operation:\n",
        "#   We will visualize each feature's behavior over time in separate plots, which will help identify patterns and outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIPU6MYbArnu"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for defining the model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Autoformer Model\n",
        "class Autoformer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout=0.1):\n",
        "        super(Autoformer, self).__init__()\n",
        "\n",
        "        # LSTM Layer: Useful for capturing temporal dependencies\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Fully connected layer: To map LSTM output to final prediction\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passing input through LSTM layer\n",
        "        out, _ = self.lstm(x)\n",
        "\n",
        "        # Output from the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Example parameters (can be tuned later)\n",
        "input_size = num_features  # Number of features (excluding 'date')\n",
        "output_size = 1  # Predicting one value per time step (e.g., next value)\n",
        "hidden_size = 64  # Number of features in the hidden state\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "dropout = 0.1  # Dropout to prevent overfitting\n",
        "\n",
        "# Instantiate the Autoformer model\n",
        "autoformer_model = Autoformer(input_size, output_size, hidden_size, num_layers, dropout)\n",
        "\n",
        "# Print model architecture\n",
        "print(autoformer_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMJ11ZYsAxL9"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   The Autoformer model is designed to capture time-series data effectively by using LSTM layers.\n",
        "#   It is chosen due to its ability to model temporal patterns and trends in data over time, which is crucial for forecasting tasks.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   The model is built using LSTM layers that capture sequential dependencies and a final fully connected layer to produce predictions.\n",
        "#   We can then use this model to forecast the future values of the time series based on historical data.\n",
        "# c) Explanation of terms used:\n",
        "#   - **LSTM (Long Short-Term Memory)**: A type of recurrent neural network (RNN) designed to handle long-range dependencies in sequential data.\n",
        "#   - **Fully Connected Layer**: A neural network layer where each neuron is connected to every neuron in the next layer.\n",
        "#   - **Dropout**: A regularization technique used to prevent overfitting by randomly \"dropping\" units during training.\n",
        "# d) What we will achieve from this operation:\n",
        "#   This will define a model that can learn temporal dependencies and make predictions for future time steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnQEbXrBBgBk"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming 'ecl_data_filled' is the dataset loaded in previous steps\n",
        "# We will use MinMaxScaler to scale the data to the range [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the data excluding the 'date' column\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(ecl_data_filled.drop(['date'], axis=1)))\n",
        "\n",
        "# Check the first few rows of the scaled data\n",
        "data_scaled.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNW5Jor4Bj4D"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   Scaling ensures that all features have a similar range, which helps improve the training performance of the model.\n",
        "#   It also prevents certain features from dominating the learning process due to differing magnitudes.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   The `MinMaxScaler` scales the values of all features between 0 and 1, ensuring consistency in the range of input data.\n",
        "# c) Explanation of terms used:\n",
        "#   - **MinMaxScaler**: A feature scaling method that rescales the data so that all feature values lie between 0 and 1.\n",
        "#   - **fit_transform()**: A method used to first compute the scaling parameters and then apply the transformation to the data.\n",
        "# d) What we will achieve from this operation:\n",
        "#   The dataset will be scaled, and all features will lie within the same range, making the model more stable and efficient during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKkd2xKjBlCu"
      },
      "outputs": [],
      "source": [
        "# Split the scaled data into features (X) and target (y)\n",
        "X_scaled = data_scaled.iloc[:, :-1]  # All columns except the last one (features)\n",
        "y_scaled = data_scaled.iloc[:, -1]  # The last column (target)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_scaled = torch.tensor(X_scaled.values, dtype=torch.float32)\n",
        "y_scaled = torch.tensor(y_scaled.values, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v_DgGcgBoSf"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   The features (X) are all columns except the target column, and the target (y) is the last column of the dataset.\n",
        "#   Converting the data into PyTorch tensors allows the model to handle the data effectively during training.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   By separating the features and target and converting them to tensors, we can efficiently pass them into the model for training.\n",
        "# c) Explanation of terms used:\n",
        "#   - **iloc**: An indexing method in pandas to select rows and columns based on integer position.\n",
        "#   - **torch.tensor()**: Converts the data into a PyTorch tensor, which is a data structure that PyTorch uses for efficient computation.\n",
        "# d) What we will achieve from this operation:\n",
        "#   We will have the features and target separated and ready to be fed into the model for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUv6H0brFeIr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming 'ecl_data_filled' is the data we have already prepared\n",
        "\n",
        "# Scaling the features (excluding the 'date' column)\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(ecl_data_filled.drop(['date'], axis=1)))\n",
        "\n",
        "# Check the shape of the scaled data\n",
        "print(data_scaled.shape)\n",
        "\n",
        "# Now we create sequences from the scaled data (X_scaled)\n",
        "sequence_length = 10  # You can adjust this value as needed\n",
        "\n",
        "# Function to create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        sequences.append(data.iloc[i:i+seq_length].values)  # Convert to numpy array\n",
        "    return torch.tensor(sequences, dtype=torch.float32)\n",
        "\n",
        "# Create sequences for the input data\n",
        "X_scaled_sequences = create_sequences(data_scaled, sequence_length)\n",
        "\n",
        "# Check the shape of the reshaped data\n",
        "print(X_scaled_sequences.shape)  # Should be (num_samples - sequence_length, sequence_length, num_features)\n",
        "\n",
        "# Define the Autoformer model\n",
        "class Autoformer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout):\n",
        "        super(Autoformer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should be of shape (batch_size, sequence_length, input_size)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
        "        return out\n",
        "\n",
        "# Initialize the Autoformer model\n",
        "autoformer_model = Autoformer(input_size=X_scaled_sequences.shape[2],\n",
        "                              output_size=1,\n",
        "                              hidden_size=64,\n",
        "                              num_layers=2,\n",
        "                              dropout=0.1)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(autoformer_model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    predictions = autoformer_model(X_scaled_sequences)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(predictions, y_scaled[sequence_length:])\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# a) Why we are using this strategy:\n",
        "#   The training loop is updated to use the reshaped data (3D tensor), and the loss is calculated based on the output of the model.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   The model will process the sequential data, and we will train it to minimize the MSE loss.\n",
        "# c) Explanation of terms used:\n",
        "#   - **y_scaled[sequence_length:]**: Since the data is reshaped into sequences, we start the target variable from the `sequence_length` index to match the corresponding sequence inputs.\n",
        "# d) What we will achieve from this operation:\n",
        "#   The model will be trained on the sequential data and learn the temporal patterns in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHp4h1xVONgu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming X_scaled_sequences and y_scaled are already defined\n",
        "\n",
        "# Initialize the Autoformer model\n",
        "autoformer_model = Autoformer(input_size=X_scaled_sequences.shape[2],\n",
        "                              output_size=1,\n",
        "                              hidden_size=64,\n",
        "                              num_layers=2,\n",
        "                              dropout=0.1)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss shaping functions (for ERM, Exponential, Constant)\n",
        "def exponential_loss(prediction, target, alpha=1.0):\n",
        "    # Exponential loss function\n",
        "    return torch.mean(torch.exp(alpha * (prediction - target)) - 1)\n",
        "\n",
        "def constant_loss(prediction, target, penalty=0.1):\n",
        "    # Constant loss function (fixed penalty)\n",
        "    return penalty * torch.mean(torch.abs(prediction - target))\n",
        "\n",
        "# Define the standard loss function (ERM - Empirical Risk Minimization)\n",
        "def standard_loss(prediction, target):\n",
        "    return torch.mean((prediction - target) ** 2)  # Example: MSE loss\n",
        "\n",
        "# Autoformer Model (same as before)\n",
        "autoformer_model = Autoformer(input_size=X_scaled_sequences.shape[2],\n",
        "                              output_size=1,\n",
        "                              hidden_size=64,\n",
        "                              num_layers=2,\n",
        "                              dropout=0.1)\n",
        "\n",
        "optimizer = optim.Adam(autoformer_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop with loss shaping constraints (ERM, Exponential, Constant)\n",
        "for epoch in range(num_epochs):\n",
        "    autoformer_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = autoformer_model(X_scaled_sequences)\n",
        "\n",
        "    # Compute the standard loss (ERM)\n",
        "    loss = standard_loss(predictions, y_scaled[sequence_length:])\n",
        "\n",
        "    # Apply Exponential loss shaping (optional)\n",
        "    exp_loss = exponential_loss(predictions, y_scaled[sequence_length:], alpha=1.0)\n",
        "\n",
        "    # Apply Constant loss shaping (optional)\n",
        "    const_loss = constant_loss(predictions, y_scaled[sequence_length:], penalty=0.1)\n",
        "\n",
        "    # Total loss = Standard Loss + Loss Shaping Constraints\n",
        "    total_loss = loss + exp_loss + const_loss\n",
        "\n",
        "    # Backward pass\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}')\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(autoformer_model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    autoformer_model.train()  # Set the model to training mode\n",
        "    # Forward pass\n",
        "    predictions = autoformer_model(X_scaled_sequences)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(predictions, y_scaled[sequence_length:])\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "autoformer_model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # No gradient calculation needed during evaluation\n",
        "    predictions = autoformer_model(X_scaled_sequences)  # Get predictions from the model\n",
        "\n",
        "# Evaluate performance using MSE and R-squared\n",
        "# Assuming y_scaled is the true target values corresponding to X_scaled_sequences\n",
        "mse = criterion(predictions, y_scaled[sequence_length:])\n",
        "r_squared = 1 - torch.sum((predictions - y_scaled[sequence_length:]) ** 2) / torch.sum((y_scaled[sequence_length:] - torch.mean(y_scaled[sequence_length:])) ** 2)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'MSE: {mse.item():.4f}')\n",
        "print(f'R-squared: {r_squared.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziIkDkCaYMr_"
      },
      "outputs": [],
      "source": [
        "# #a) Why we are using this strategy:\n",
        "# In the machine learning domain, the process of training models involves minimizing a loss function to improve prediction accuracy.\n",
        "# To ensure that our model not only fits the data well but also generalizes effectively, we add extra components like regularization and loss shaping constraints.\n",
        "# This is particularly important when the model starts to overfit or underfit the training data. By adding additional loss terms like the Exponential and Constant loss functions,\n",
        "# we are imposing constraints that guide the model to learn in a more structured and robust manner, improving its ability to predict unseen data accurately.\n",
        "# Regularization terms help in preventing the model from becoming too complex or overfitting to the noise in the training set.\n",
        "\n",
        "# #b) How these codes, functions, and operations will solve our purpose:\n",
        "# The training loop has been designed to optimize the model by minimizing a combined loss function that includes:\n",
        "#   1. **Standard Loss (ERM)**: This is the core loss function, typically Mean Squared Error (MSE), that measures how well the model's predictions match the true target values.\n",
        "#   2. **Exponential Loss**: This additional loss component helps impose a smoother learning curve by penalizing large deviations from the target values exponentially, preventing erratic behavior in predictions.\n",
        "#   3. **Constant Loss**: The constant loss introduces a fixed penalty based on the absolute difference between predictions and targets. It forces the model to balance between fitting the data and staying within a reasonable range of values.\n",
        "# By incorporating these additional losses, the model will be encouraged to learn in a more structured manner, avoiding overfitting and ensuring better generalization.\n",
        "\n",
        "# #c) Explanation of the terms used:\n",
        "#   1. **Standard Loss (ERM)**: Empirical Risk Minimization (ERM) is a strategy where the model aims to minimize the discrepancy (error) between predicted values and actual target values on the training data. MSE (Mean Squared Error) is a common example of this loss function.\n",
        "#   2. **Exponential Loss**: This is a type of loss where the penalty increases exponentially with the magnitude of the error. It’s useful in scenarios where larger errors should be penalized more heavily.\n",
        "#   3. **Constant Loss**: The constant loss applies a fixed penalty (a constant) to the error, irrespective of its size. This is a way of constraining the model’s prediction behavior.\n",
        "#   4. **Backpropagation**: This is the process of computing gradients of the loss function with respect to the model's parameters and updating the parameters accordingly using an optimization algorithm like Adam.\n",
        "\n",
        "# #d) What will we achieve from this operation:\n",
        "# By combining these losses, we are achieving several things:\n",
        "#   - **Better Generalization**: The additional loss terms help the model generalize better by adding structure to the learning process.\n",
        "#   - **Avoiding Overfitting**: The regularization effects of the exponential and constant losses prevent the model from memorizing the training data and instead encourage it to find patterns that are more broadly applicable.\n",
        "#   - **Improved Robustness**: The combined loss function ensures the model learns not only from the data but also from additional constraints, making it more stable and robust to unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJJ95YygBt_e"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   The training loop is designed to optimize the model using the Adam optimizer and minimize the error using the MSE loss function.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   The model will learn from the input data and update its weights during each epoch to minimize the error and improve predictions.\n",
        "# c) Explanation of terms used:\n",
        "#   - **optimizer.zero_grad()**: Clears the gradients of all optimized tensors.\n",
        "#   - **loss.backward()**: Computes the gradient of the loss with respect to the parameters.\n",
        "#   - **optimizer.step()**: Updates the model parameters based on the gradients.\n",
        "# d) What we will achieve from this operation:\n",
        "#   The model will be trained over 50 epochs, and we will observe the loss decrease, indicating that the model is learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eku7KPEdB1Tk"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   Evaluating the model on the test data allows us to assess its generalization performance and accuracy.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   We calculate the MSE and R-squared scores, which are useful metrics for regression tasks, to quantify the model's performance.\n",
        "# c) Explanation of terms used:\n",
        "#   - **eval()**: Puts the model in evaluation mode, disabling certain features like dropout.\n",
        "#   - **mean_squared_error()**: A function from sklearn that computes the mean squared error between predicted and actual values.\n",
        "#   - **r2_score()**: A function from sklearn that calculates R-squared, indicating how well the model explains the variance in the data.\n",
        "# d) What we will achieve from this operation:\n",
        "#   We will evaluate the model's performance and get a sense of how well it predicts the target variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHVf5XMBA2Uo"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error for regression tasks\n",
        "optimizer = optim.Adam(autoformer_model.parameters(), lr=0.001)  # Adam optimizer for training\n",
        "\n",
        "# Prepare the data for training (Example: You would typically split the data into training and validation sets)\n",
        "X_train, y_train = data_scaled.iloc[:, :-1], data_scaled.iloc[:, -1]  # Features and target\n",
        "X_train = torch.tensor(X_train.values, dtype=torch.float32)  # Convert to torch tensor\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "# Training loop (example)\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    predictions = autoformer_model(X_train)\n",
        "    loss = criterion(predictions, y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training loss every 10 epochs\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljS-FSHvA9Cz"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   The Adam optimizer is used for efficient training, while the Mean Squared Error (MSE) loss function is suitable for regression tasks.\n",
        "#   We train the model over multiple epochs to optimize the weights of the network based on the training data.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   By training the model over several epochs, it will learn the patterns and dependencies in the time series data, enabling accurate forecasts.\n",
        "#   The optimizer updates the model weights to minimize the error, and the loss function quantifies the prediction error.\n",
        "# c) Explanation of terms used:\n",
        "#   - **Adam optimizer**: A popular optimization algorithm used for training neural networks that adapts the learning rate during training.\n",
        "#   - **MSELoss**: A loss function that calculates the average squared differences between predicted and actual values, used for regression tasks.\n",
        "#   - **Epochs**: A full pass over the entire dataset during training.\n",
        "# d) What we will achieve from this operation:\n",
        "#   The model will be trained and optimized to learn the patterns from the time series data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQNLBarNBAPF"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model (Example: on validation set or test set)\n",
        "# Here, we would split the data into training and validation/test sets\n",
        "X_test, y_test = data_scaled.iloc[:, :-1], data_scaled.iloc[:, -1]\n",
        "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# Test the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # No need to calculate gradients during testing\n",
        "    predictions = model(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test.numpy(), predictions.numpy())\n",
        "r2 = r2_score(y_test.numpy(), predictions.numpy())\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'R-squared: {r2:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC9ncHXbBC1o"
      },
      "outputs": [],
      "source": [
        "# a) Why we are using this strategy:\n",
        "#   After training, we need to evaluate the model to assess how well it performs on unseen data (test set).\n",
        "#   Metrics like Mean Squared Error (MSE) and R-squared help quantify model performance.\n",
        "# b) How these codes will solve our purpose:\n",
        "#   The code calculates the MSE and R-squared scores to evaluate the predictive accuracy of the trained model.\n",
        "# c) Explanation of terms used:\n",
        "#   - **Mean Squared Error (MSE)**: A common evaluation metric for regression tasks, representing the average squared difference between predicted and true values.\n",
        "#   - **R-squared**: A measure of how well the model explains the variance in the target variable, with 1 indicating perfect prediction.\n",
        "# d) What we will achieve from this operation:\n",
        "#   We will assess how well the model is performing on the test set and understand its predictive capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMaIxp3KBFSN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Custom Loss Function with Shaping Constraints (e.g., L2 regularization or custom constraints)\n",
        "class LossShaping(nn.Module):\n",
        "    def __init__(self, alpha=0.01):\n",
        "        super(LossShaping, self).__init__()\n",
        "        self.alpha = alpha  # Regularization strength\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        mse_loss = nn.MSELoss()(predictions, targets)\n",
        "        # Example constraint: L2 regularization on the weights\n",
        "        l2_reg = torch.sum(torch.square(predictions))\n",
        "        total_loss = mse_loss + self.alpha * l2_reg\n",
        "        return total_loss\n",
        "\n",
        "# Loss shaping implementation with the Autoformer model\n",
        "autoformer_model = Autoformer(input_size=X_scaled_sequences.shape[2],\n",
        "                              output_size=1,\n",
        "                              hidden_size=64,\n",
        "                              num_layers=2,\n",
        "                              dropout=0.1)\n",
        "\n",
        "optimizer = optim.Adam(autoformer_model.parameters(), lr=0.001)\n",
        "loss_shaping = LossShaping(alpha=0.01)\n",
        "\n",
        "# Training loop with loss shaping\n",
        "for epoch in range(num_epochs):\n",
        "    autoformer_model.train()\n",
        "    predictions = autoformer_model(X_scaled_sequences)\n",
        "    loss = loss_shaping(predictions, y_scaled[sequence_length:])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        # Initialize separate lists to store losses for each strategy\n",
        "erm_losses = []\n",
        "exponential_losses = []\n",
        "constant_losses = []\n",
        "\n",
        "# Example: Training with ERM\n",
        "for epoch in range(num_epochs):\n",
        "    autoformer_model.train()\n",
        "    predictions = autoformer_model(X_scaled_sequences)\n",
        "    loss = erm_loss_fn(predictions, y_scaled[sequence_length:])  # ERM loss function\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_value = loss.item()\n",
        "    erm_losses.append(loss_value)\n",
        "\n",
        "# Example: Training with Exponential Loss Shaping\n",
        "for epoch in range(num_epochs):\n",
        "    autoformer_model.train()\n",
        "    predictions = autoformer_model(X_scaled_sequences)\n",
        "    loss = exponential_loss_fn(predictions, y_scaled[sequence_length:])  # Exponential loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_value = loss.item()\n",
        "    exponential_losses.append(loss_value)\n",
        "\n",
        "# Example: Training with Constant Loss Shaping\n",
        "for epoch in range(num_epochs):\n",
        "    autoformer_model.train()\n",
        "    predictions = autoformer_model(X_scaled_sequences)\n",
        "    loss = constant_loss_fn(predictions, y_scaled[sequence_length:])  # Constant loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_value = loss.item()\n",
        "    constant_losses.append(loss_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9LFybV5RV_2"
      },
      "outputs": [],
      "source": [
        "# a) Loss shaping constraints are used to guide the model to avoid overfitting or undesirable\n",
        "# behavior. The custom loss function combines MSE with an L2 regularization term (which penalizes\n",
        "# large weights), thus controlling model complexity.\n",
        "\n",
        "# b) By combining MSE with an L2 regularization term, we ensure the model doesn't overfit the data.\n",
        "# The loss shaping function adjusts the model’s loss to consider both prediction accuracy (MSE)\n",
        "# and weight regularization (L2 term). This helps in obtaining better generalization.\n",
        "\n",
        "# c) `LossShaping` is a custom loss function class that takes predictions and targets as inputs.\n",
        "# It computes MSE and adds an L2 penalty on the predictions. The regularization strength is controlled\n",
        "# by the `alpha` parameter.\n",
        "\n",
        "# d) By implementing loss shaping, we encourage the model to generalize better by preventing\n",
        "# it from overfitting to the training data. The L2 regularization term penalizes large weights\n",
        "# and helps improve the model's performance on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mISKr_hVRb4F"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have predictions and actual values\n",
        "predictions = autoformer_model(X_scaled_sequences).detach().numpy()\n",
        "actual_values = y_scaled[sequence_length:].numpy()\n",
        "\n",
        "# Plot predictions vs actual values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(actual_values, label='Actual')\n",
        "plt.plot(predictions, label='Predicted', linestyle='--')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf0fjnr0RiEf"
      },
      "outputs": [],
      "source": [
        "# a) Visualizing predictions versus actual values allows us to assess the model’s performance.\n",
        "# The graph helps us understand how well the model is generalizing and whether it’s overfitting\n",
        "# or underfitting.\n",
        "\n",
        "# b) By plotting the actual and predicted values, we can visually inspect how close the model’s\n",
        "# predictions are to the true values. The dashed line represents predictions, while the solid line\n",
        "# represents the true values, allowing easy comparison.\n",
        "\n",
        "# c) `plt.plot()` is used to plot the data points, and `plt.legend()` adds labels to the plot for\n",
        "# easy interpretation. `plt.show()` displays the plot.\n",
        "\n",
        "# d) The result is a graphical comparison that shows how well the model performs in forecasting\n",
        "# the time series. A close alignment of predicted and actual values indicates a good model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOn8CftnRlwv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define the Transformer model class\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_heads, n_layers, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads),\n",
        "            num_layers=n_layers\n",
        "        )put_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, out\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoSzKqTmT2yC"
      },
      "outputs": [],
      "source": [
        "# TransformerModel Class\n",
        "# a) This class defines the Transformer architecture, which consists of an encoder layer followed by a fully connected (fc) layer.\n",
        "# b) The encoder processes the input data, extracting relevant features, while the fully connected layer makes predictions from these features.\n",
        "# c) nn.TransformerEncoderLayer: This is the building block of the transformer encoder. It takes in input and outputs transformed feature vectors.\n",
        "# d) The model ends with a Linear layer, which maps the encoded features to the output dimension (predictions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtFVcjLlUNDl"
      },
      "outputs": [],
      "source": [
        "# Step 1: Set up the training loop\n",
        "def train_transformer(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            output = model(X_batch)  # Forward pass\n",
        "            loss = criterion(output, y_batch)  # Calculate loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update parameters\n",
        "            epoch_loss += loss.item()  # Accumulate loss\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9cF82KnUQ9G"
      },
      "outputs": [],
      "source": [
        "# Training Loop for Transformer\n",
        "# a) We define a function that will train the transformer model for a specified number of epochs, using mini-batches of data from the train_loader.\n",
        "# b) The function processes the data in batches, computes the forward pass, calculates the loss, performs backpropagation, and updates the model parameters.\n",
        "# c) We use the criterion (loss function) to measure how well the model's predictions match the actual values. The optimizer is responsible for updating the model's parameters based on the loss.\n",
        "# d) By running this, we train the transformer model, iteratively improving its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2YHr-f0UdTU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "# Evaluation function for testing the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():  # No gradient calculation needed during evaluation\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            predictions = model(X_batch)\n",
        "            all_predictions.extend(predictions.numpy())  # Collect predictions\n",
        "            all_labels.extend(y_batch.numpy())  # Collect actual values\n",
        "    mse = mean_squared_error(all_labels, all_predictions)  # Calculate MSE\n",
        "    rmse = math.sqrt(mse)  # Calculate RMSE\n",
        "    return mse, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89tu6tRhUhhF"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "# a) This function evaluates the performance of the model using test data, by calculating MSE and RMSE.\n",
        "# b) We compute predictions on the test data and compare them to the actual values using mean_squared_error.\n",
        "# c) The MSE measures the average squared difference between the predicted and actual values. RMSE is just the square root of MSE.\n",
        "# d) By running this, we get a numerical evaluation of the model's performance on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uDksS8mUlMm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize predictions vs actual values\n",
        "def plot_predictions(actual, predicted):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(actual, label='Actual', color='blue')\n",
        "    plt.plot(predicted, label='Predicted', color='red')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title('Predicted vs Actual')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9yx85abUqgs"
      },
      "outputs": [],
      "source": [
        "# Set up the data loader for training and testing (make sure your data is in a proper format)\n",
        "train_loader = torch.utils.data.DataLoader(data_filled, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(data_filled, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "model = TransformerModel(input_dim=29, hidden_dim=64, n_heads=4, n_layers=2, output_dim=1)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_transformer(model, train_loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "mse, rmse = evaluate_model(model, test_loader)\n",
        "\n",
        "# Visualize the results\n",
        "plot_predictions(all_labels, all_predictions)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTmvLcnpU3Gb"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation Results\n",
        "# a) By training the model and evaluating it on the test set, we compute MSE and RMSE, which provide insight into the model's performance.\n",
        "# b) These metrics help us quantify how well our transformer model is predicting the target values.\n",
        "# c) MSE penalizes large errors more heavily, while RMSE gives a more interpretable value in the same units as the original data.\n",
        "# d) The evaluation results guide us in understanding whether the model is suitable for our forecasting task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuEPR28oY_2Q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize lists to store loss values for ERM, Exponential, and Constant\n",
        "erm_losses = []\n",
        "exponential_losses = []\n",
        "constant_losses = []\n",
        "\n",
        "# Simulate loss values over epochs (replace with actual loss values from your training loop)\n",
        "# For illustration purposes, we'll generate some example loss values that might represent each type of loss function.\n",
        "# These should be replaced with actual loss calculations from your training loop.\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Example values for ERM, Exponential, and Constant losses\n",
        "    erm_loss = np.random.uniform(0.1, 0.3) + 0.05 * np.sin(epoch / 10)  # Simulated ERM loss\n",
        "    exponential_loss = np.random.uniform(0.1, 0.3) + 0.07 * np.exp(-epoch / 15)  # Simulated Exponential loss\n",
        "    constant_loss = 0.2 + 0.03 * epoch  # Simulated Constant loss\n",
        "\n",
        "    # Append to lists\n",
        "    erm_losses.append(erm_loss)\n",
        "    exponential_losses.append(exponential_loss)\n",
        "    constant_losses.append(constant_loss)\n",
        "\n",
        "# Plotting the losses\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# ERM Loss\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(range(num_epochs), erm_losses, color='blue', label='ERM Loss')\n",
        "plt.title('ERM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Exponential Loss\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(range(num_epochs), exponential_losses, color='red', label='Exponential Loss')\n",
        "plt.title('Exponential Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Constant Loss\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(range(num_epochs), constant_losses, color='green', label='Constant Loss')\n",
        "plt.title('Constant Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1_0gPQ6ZR2E"
      },
      "outputs": [],
      "source": [
        "# (a) We are importing the libraries \"matplotlib.pyplot\" and \"numpy\" for visualizations and numerical operations.\n",
        "# (b) These libraries allow us to create plots and handle the numerical computations necessary to simulate or analyze the loss functions over epochs.\n",
        "# (c) \"matplotlib.pyplot\" is used for creating visual plots, while \"numpy\" is used to generate arrays and perform mathematical operations.\n",
        "# (d) With these libraries, we can plot the behavior of loss functions (ERM, Exponential, and Constant) over time and handle the data calculations required for the loss curves.\n",
        "# (a) We are simulating the loss values for the ERM, Exponential, and Constant loss functions over 100 epochs.\n",
        "# (b) These simulated values are placeholders, and during actual training, these loss values would come from the loss function applied to model predictions and targets.\n",
        "# (c) The \"erm_losses\", \"exponential_losses\", and \"constant_losses\" lists will store the loss values for each function over time.\n",
        "# (d) By simulating these loss values, we can visualize the performance of different loss functions (ERM, Exponential, Constant) over epochs and assess their trends.\n",
        "# (a) We are plotting the loss values for ERM, Exponential, and Constant loss functions in separate subplots for easy comparison.\n",
        "# (b) By using subplots, we allow a side-by-side view of each loss function's evolution over time, enabling a more intuitive comparison.\n",
        "# (c) \"plt.subplot()\" creates a subplot layout, while \"plt.plot()\" plots the loss values for each function. \"plt.title()\" adds titles, \"plt.xlabel()\" and \"plt.ylabel()\" label the axes, and \"plt.legend()\" adds a legend to identify each loss curve.\n",
        "# (d) This step will generate a visualization where you can observe how each loss function behaves over time (in terms of convergence or fluctuation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb8lchg0cRNW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Use distinct colors and labels for clarity\n",
        "colors = {\n",
        "    \"ERM\": \"deepskyblue\",        # Cool blue\n",
        "    \"Exponential\": \"darkorange\", # Vibrant orange\n",
        "    \"Constant\": \"forestgreen\"    # Earthy green\n",
        "}\n",
        "\n",
        "#  Create the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "#  Plot each loss trend\n",
        "plt.plot(erm_losses, label='ERM Loss', color=colors[\"ERM\"], linewidth=2)\n",
        "plt.plot(exp_losses, label='Exponential Loss', color=colors[\"Exponential\"], linestyle='--', linewidth=2)\n",
        "plt.plot(const_losses, label='Constant Loss', color=colors[\"Constant\"], linestyle='-.', linewidth=2)\n",
        "\n",
        "#  Beautify the plot\n",
        "plt.title('Autoformer: Training Loss Comparison Across Loss Shaping Strategies', fontsize=16)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Loss Value (MSE + Regularization)', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, linestyle=':', alpha=0.7)\n",
        "\n",
        "#  Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFAJFtNndkCb"
      },
      "outputs": [],
      "source": [
        "# Setting random seeds for reproducibility\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)  # If using GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-sU1SmFfkPG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfkwq2f6zOGCQWotuYCT4m",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPhgHM36+Z3dFmlZqsI/lm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basugautam/Reproducibility-Challenge-Project/blob/Architecture-Files/10_rethinkig_error_aggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDk4NzuNIUGm"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ðŸ”¹ Step 1: Load and Preprocess Dataset\n",
        "dataset_path = r\"C:\\LCTSF\\Dataset\\timeseries_dataset.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Extract time-series values\n",
        "data = df['value'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# ðŸ”¹ Step 2: Create Time-Series Sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length, 0])\n",
        "        y.append(data[i + seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "sequence_length = 50\n",
        "X, y = create_sequences(data_scaled, sequence_length)\n",
        "\n",
        "# Split into Training & Testing Sets\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)  # Add feature dimension\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "# Create PyTorch Dataset\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Define DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TimeSeriesDataset(X_train_torch, y_train_torch), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TimeSeriesDataset(X_test_torch, y_test_torch), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ðŸ”¹ Step 3: Define Transformer Model with Weighted Error Aggregation\n",
        "class TransformerForecasting(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout=0.1):\n",
        "        super(TransformerForecasting, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, model_dim)  # Project input to model dimension\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(model_dim, 1)  # Final prediction layer\n",
        "        self.weight_vector = nn.Parameter(torch.linspace(1.0, 2.0, sequence_length))  # Learnable weights for time-step importance\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        output = self.fc(x[:, -1, :])  # Use last time step for prediction\n",
        "        return output\n",
        "\n",
        "# Instantiate Transformer Model\n",
        "input_dim = 1\n",
        "model_dim = 64\n",
        "num_heads = 4\n",
        "num_layers =_\n"
      ]
    }
  ]
}
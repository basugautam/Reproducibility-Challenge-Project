{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2hZF+eLa2TMFGqrWZCfUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basugautam/Reproducibility-Challenge-Project/blob/Architecture-Files/Complete_project_codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTEZyOKaRh1A",
        "outputId": "45ca6202-cf41-4911-d488-969dd0bac97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üîç Preview raw file structure:\n",
            "         0     1     2                 3            4   \\\n",
            "0       NaN   NaN   NaN               NaN          NaN   \n",
            "1      Name  GENC  Year  Total Population  Growth Rate   \n",
            "2   -> 2024   NaN   NaN                --           --   \n",
            "3    Canada    CA  2024        38,904,514         0.72   \n",
            "4   -> 2025   NaN   NaN                --           --   \n",
            "5    Canada    CA  2025        39,187,155         0.73   \n",
            "6   -> 2026   NaN   NaN                --           --   \n",
            "7    Canada    CA  2026        39,465,520         0.69   \n",
            "8   -> 2027   NaN   NaN                --           --   \n",
            "9    Canada    CA  2027        39,730,162         0.65   \n",
            "\n",
            "                               5                     6   \\\n",
            "0                             NaN                   NaN   \n",
            "1  Population Density (per sq km)  Total Fertility Rate   \n",
            "2                              --                    --   \n",
            "3                             4.3                  1.44   \n",
            "4                              --                    --   \n",
            "5                             4.3                  1.43   \n",
            "6                              --                    --   \n",
            "7                             4.3                  1.43   \n",
            "8                              --                    --   \n",
            "9                             4.4                  1.42   \n",
            "\n",
            "                         7                       8   \\\n",
            "0                       NaN                     NaN   \n",
            "1  Life Expectancy at Birth  Under-5 Mortality Rate   \n",
            "2                        --                      --   \n",
            "3                      83.9                     4.8   \n",
            "4                        --                      --   \n",
            "5                      84.8                     4.4   \n",
            "6                        --                      --   \n",
            "7                      85.0                     4.4   \n",
            "8                        --                      --   \n",
            "9                      85.2                     4.3   \n",
            "\n",
            "                            9                                 10  \\\n",
            "0                          NaN                  Dependency Ratio   \n",
            "1  Sex Ratio of the Population  Youth and Old Age (0-14 and 65+)   \n",
            "2                           --                                --   \n",
            "3                         0.99                              56.8   \n",
            "4                           --                                --   \n",
            "5                         0.99                              57.7   \n",
            "6                           --                                --   \n",
            "7                         0.99                              58.7   \n",
            "8                           --                                --   \n",
            "9                         0.99                              59.7   \n",
            "\n",
            "             11             12          13    14      15  \n",
            "0           NaN            NaN  Median Age   NaN     NaN  \n",
            "1  Youth (0-14)  Old Age (65+)  Both Sexes  Male  Female  \n",
            "2            --             --          --    --      --  \n",
            "3          23.9           32.9        42.5  43.9    41.2  \n",
            "4            --             --          --    --      --  \n",
            "5          23.8           33.9        42.8  44.1    41.4  \n",
            "6            --             --          --    --      --  \n",
            "7          23.7           35.0        43.0  44.4    41.7  \n",
            "8            --             --          --    --      --  \n",
            "9          23.6           36.1        43.3  44.7    41.9  \n",
            "‚úÖ Cleaned CSV saved here:\n",
            "/content/drive/My Drive/timeseries_data_cleaned.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18b8ca472ce9>:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "# üìå Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# üìå Step 2: Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# üìå Step 3: Define original and new paths\n",
        "original_path = '/content/drive/My Drive/timeseries_data.csv.csv'  # Update this if needed\n",
        "cleaned_path = '/content/drive/My Drive/timeseries_data_cleaned.csv'  # This will be our new clean CSV\n",
        "\n",
        "# üìå Step 4: Load the raw file without assuming header\n",
        "df_raw = pd.read_csv(original_path, header=None)\n",
        "print(\"üîç Preview raw file structure:\")\n",
        "print(df_raw.head(10))  # View first 10 rows to understand structure\n",
        "\n",
        "# üìå Step 5: Extract real header row and data\n",
        "# We assume row 0 is column names and actual data starts from row 1\n",
        "df_cleaned = pd.read_csv(original_path, skiprows=1)\n",
        "\n",
        "# üìå Step 6: Rename first column as 'Date' if it's a timestamp\n",
        "df_cleaned.rename(columns={df_cleaned.columns[0]: 'Date'}, inplace=True)\n",
        "\n",
        "# üìå Step 7: Convert 'Date' column to datetime format\n",
        "df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'], errors='coerce')\n",
        "\n",
        "# üìå Step 8: Drop rows where Date is NaT (invalid timestamp)\n",
        "df_cleaned = df_cleaned.dropna(subset=['Date'])\n",
        "\n",
        "# üìå Step 9: Save the cleaned data to a new file\n",
        "df_cleaned.to_csv(cleaned_path, index=False)\n",
        "print(f\"‚úÖ Cleaned CSV saved here:\\n{cleaned_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå Step 1: Load cleaned CSV from Google Drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load CSV\n",
        "file_path = '/content/drive/My Drive/timeseries_data_cleaned.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Inspect and rename columns if needed\n",
        "print(\"‚úÖ Column Names:\", df.columns.tolist())\n",
        "\n",
        "# Try to find the correct datetime column\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' and df[col].str.contains(r'\\d{4}', na=False).any():\n",
        "        df.rename(columns={col: 'Date'}, inplace=True)\n",
        "        break\n",
        "\n",
        "# Step 3: Convert 'Date' column to datetime and set index\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "df = df.set_index('Date')\n",
        "df = df.sort_index()\n",
        "df = df.dropna()\n",
        "\n",
        "# Step 4: Resample yearly (if needed)\n",
        "df_resampled = df.resample('YE').mean().interpolate()\n",
        "\n",
        "# Step 5: Create sliding windows\n",
        "input_window = 5       # past 5 years\n",
        "forecast_horizon = 3   # next 3 years\n",
        "\n",
        "X, Y = [], []\n",
        "values = df_resampled.values\n",
        "\n",
        "for i in range(len(values) - input_window - forecast_horizon):\n",
        "    x_seq = values[i : i + input_window]\n",
        "    y_seq = values[i + input_window : i + input_window + forecast_horizon]\n",
        "    X.append(x_seq)\n",
        "    Y.append(y_seq)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# Step 6: Save windows to Google Drive (optional)\n",
        "np.save('/content/drive/My Drive/X_windows.npy', X)\n",
        "np.save('/content/drive/My Drive/Y_windows.npy', Y)\n",
        "\n",
        "# ‚úÖ Output shapes\n",
        "print(\"‚úÖ Data ready for modeling!\")\n",
        "print(f\"X shape (samples, {input_window}, features):\", X.shape)\n",
        "print(f\"Y shape (samples, {forecast_horizon}, features):\", Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgeChG6GTb1t",
        "outputId": "032920ab-275b-48de-d768-0ef21df9d016"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Column Names: ['Date', 'GENC', 'Year', 'Total Population', 'Growth Rate', 'Population Density (per sq km)', 'Total Fertility Rate', 'Life Expectancy at Birth', 'Under-5 Mortality Rate', 'Sex Ratio of the Population', 'Youth and Old Age (0-14 and 65+)', 'Youth (0-14)', 'Old Age (65+)', 'Both Sexes', 'Male', 'Female']\n",
            "‚úÖ Data ready for modeling!\n",
            "X shape (samples, 5, features): (0,)\n",
            "Y shape (samples, 3, features): (0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom forecast horizons (in years)\n",
        "forecast_horizons = [1, 5, 10, 20]\n",
        "input_window = 5\n",
        "\n",
        "X_multi, Y_multi = [], []\n",
        "\n",
        "for i in range(len(df_resampled) - max(forecast_horizons) - input_window):\n",
        "    x_seq = df_resampled.iloc[i : i + input_window].values\n",
        "    y_seq = []\n",
        "    for h in forecast_horizons:\n",
        "        y = df_resampled.iloc[i + input_window + h - 1].values  # get the year at t+h\n",
        "        y_seq.append(y)\n",
        "    X_multi.append(x_seq)\n",
        "    Y_multi.append(y_seq)\n",
        "\n",
        "X_multi = np.array(X_multi)\n",
        "Y_multi = np.array(Y_multi)\n",
        "\n",
        "# Save these for modeling\n",
        "np.save('/content/drive/My Drive/X_multi.npy', X_multi)\n",
        "np.save('/content/drive/My Drive/Y_multi.npy', Y_multi)\n",
        "\n",
        "print(\"‚úÖ Multi-horizon windows ready!\")\n",
        "print(f\"X_multi shape: {X_multi.shape} ‚Üí (samples, {input_window}, features)\")\n",
        "print(f\"Y_multi shape: {Y_multi.shape} ‚Üí (samples, {len(forecast_horizons)}, features)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAAzwyekT44S",
        "outputId": "9a98552f-7f8d-4ee6-9b6f-206421d07b29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Multi-horizon windows ready!\n",
            "X_multi shape: (0,) ‚Üí (samples, 5, features)\n",
            "Y_multi shape: (0,) ‚Üí (samples, 4, features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üü¶ Step 1: Import necessary PyTorch libraries\n",
        "# üîµ (a#) We are importing these to build neural networks and define loss functions\n",
        "# üîµ (b#) PyTorch's torch.nn module provides tools to build models and custom loss functions\n",
        "# üîµ (c#) `nn` is a shorthand alias for `torch.nn` to keep code cleaner\n",
        "# üîµ (d#) With these, we can define and use custom regularized loss for multi-horizon consistency\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# üü® Step 2: Define a custom Weighted MSE Loss function\n",
        "# üîµ (a#) We want different penalties for errors at different forecast horizons\n",
        "# üîµ (b#) Weighted loss lets us emphasize closer or farther predictions\n",
        "# üîµ (c#) This subclass overrides `forward()` to compute weighted MSE\n",
        "# üîµ (d#) This allows training the model with greater control over forecasting behavior\n",
        "\n",
        "class WeightedMSELoss(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super(WeightedMSELoss, self).__init__()\n",
        "        self.weights = torch.tensor(weights, dtype=torch.float32).view(1, -1)  # Reshape to broadcast\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Ensure input shapes are compatible\n",
        "        assert predictions.shape == targets.shape, \"Prediction and target must match shape\"\n",
        "\n",
        "        # Compute squared errors\n",
        "        squared_error = (predictions - targets) ** 2\n",
        "\n",
        "        # Apply weights\n",
        "        weighted_error = squared_error * self.weights\n",
        "\n",
        "        # Return mean of weighted error\n",
        "        return weighted_error.mean()\n"
      ],
      "metadata": {
        "id": "wfae56HIoilX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üü¶ Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# üü• Step 2: Upload file manually if not using Google Drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# üü© Step 3: Load the uploaded file\n",
        "# üîµ (a#) We are loading the dataset to extract the time series for modeling\n",
        "# üîµ (b#) This will allow us to apply forecasting techniques using the data\n",
        "# üîµ (c#) `pd.read_csv()` loads CSV data into a DataFrame, which is a table-like data structure\n",
        "# üîµ (d#) This gives us access to the clean, structured input to process for deep learning\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "1IR__pNUnGsk",
        "outputId": "df58433f-1ab1-4045-b48a-32018abd2593"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c83e775e-428e-45ae-9142-2fe5d0bdd389\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c83e775e-428e-45ae-9142-2fe5d0bdd389\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving timeseries_data.csv.csv to timeseries_data.csv (1).csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Unnamed: 0 Unnamed: 1 Unnamed: 2        Unnamed: 3   Unnamed: 4  \\\n",
              "0       Name       GENC       Year  Total Population  Growth Rate   \n",
              "1    -> 2024        NaN        NaN                --           --   \n",
              "2     Canada         CA       2024        38,904,514         0.72   \n",
              "3    -> 2025        NaN        NaN                --           --   \n",
              "4     Canada         CA       2025        39,187,155         0.73   \n",
              "\n",
              "                       Unnamed: 5            Unnamed: 6  \\\n",
              "0  Population Density (per sq km)  Total Fertility Rate   \n",
              "1                              --                    --   \n",
              "2                             4.3                  1.44   \n",
              "3                              --                    --   \n",
              "4                             4.3                  1.43   \n",
              "\n",
              "                 Unnamed: 7              Unnamed: 8  \\\n",
              "0  Life Expectancy at Birth  Under-5 Mortality Rate   \n",
              "1                        --                      --   \n",
              "2                      83.9                     4.8   \n",
              "3                        --                      --   \n",
              "4                      84.8                     4.4   \n",
              "\n",
              "                    Unnamed: 9                  Dependency Ratio  \\\n",
              "0  Sex Ratio of the Population  Youth and Old Age (0-14 and 65+)   \n",
              "1                           --                                --   \n",
              "2                         0.99                              56.8   \n",
              "3                           --                                --   \n",
              "4                         0.99                              57.7   \n",
              "\n",
              "    Unnamed: 11    Unnamed: 12  Median Age Unnamed: 14 Unnamed: 15  \n",
              "0  Youth (0-14)  Old Age (65+)  Both Sexes        Male      Female  \n",
              "1            --             --          --          --          --  \n",
              "2          23.9           32.9        42.5        43.9        41.2  \n",
              "3            --             --          --          --          --  \n",
              "4          23.8           33.9        42.8        44.1        41.4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-daba44d8-df92-42fb-9bc2-56b16054710f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Dependency Ratio</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "      <th>Median Age</th>\n",
              "      <th>Unnamed: 14</th>\n",
              "      <th>Unnamed: 15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Name</td>\n",
              "      <td>GENC</td>\n",
              "      <td>Year</td>\n",
              "      <td>Total Population</td>\n",
              "      <td>Growth Rate</td>\n",
              "      <td>Population Density (per sq km)</td>\n",
              "      <td>Total Fertility Rate</td>\n",
              "      <td>Life Expectancy at Birth</td>\n",
              "      <td>Under-5 Mortality Rate</td>\n",
              "      <td>Sex Ratio of the Population</td>\n",
              "      <td>Youth and Old Age (0-14 and 65+)</td>\n",
              "      <td>Youth (0-14)</td>\n",
              "      <td>Old Age (65+)</td>\n",
              "      <td>Both Sexes</td>\n",
              "      <td>Male</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-&gt; 2024</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Canada</td>\n",
              "      <td>CA</td>\n",
              "      <td>2024</td>\n",
              "      <td>38,904,514</td>\n",
              "      <td>0.72</td>\n",
              "      <td>4.3</td>\n",
              "      <td>1.44</td>\n",
              "      <td>83.9</td>\n",
              "      <td>4.8</td>\n",
              "      <td>0.99</td>\n",
              "      <td>56.8</td>\n",
              "      <td>23.9</td>\n",
              "      <td>32.9</td>\n",
              "      <td>42.5</td>\n",
              "      <td>43.9</td>\n",
              "      <td>41.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-&gt; 2025</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "      <td>--</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Canada</td>\n",
              "      <td>CA</td>\n",
              "      <td>2025</td>\n",
              "      <td>39,187,155</td>\n",
              "      <td>0.73</td>\n",
              "      <td>4.3</td>\n",
              "      <td>1.43</td>\n",
              "      <td>84.8</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0.99</td>\n",
              "      <td>57.7</td>\n",
              "      <td>23.8</td>\n",
              "      <td>33.9</td>\n",
              "      <td>42.8</td>\n",
              "      <td>44.1</td>\n",
              "      <td>41.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-daba44d8-df92-42fb-9bc2-56b16054710f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-daba44d8-df92-42fb-9bc2-56b16054710f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-daba44d8-df92-42fb-9bc2-56b16054710f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bbcd3ae4-be00-4fce-97cb-0a8721f2d4ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bbcd3ae4-be00-4fce-97cb-0a8721f2d4ce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bbcd3ae4-be00-4fce-97cb-0a8721f2d4ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 55,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \" -> 2049\",\n          \" -> 2038\",\n          \" -> 2034\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"CA\",\n          \"GENC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"2032\",\n          \"2048\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"43,285,886\",\n          \"41,927,305\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"0.20\",\n          \"0.38\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 5\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"--\",\n          \"4.6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 6\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Total Fertility Rate\",\n          \"--\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 7\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"88.6\",\n          \"87.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 8\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"Under-5 Mortality Rate\",\n          \"--\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 9\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"--\",\n          \"0.98\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dependency Ratio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"68.5\",\n          \"64.4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 11\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"Youth (0-14)\",\n          \"21.8\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 12\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"47.7\",\n          \"43.1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Median Age\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"48.9\",\n          \"46.5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 14\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"50.2\",\n          \"47.8\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 15\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"47.6\",\n          \"45.2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üü¶ Print all available columns so we know what to use\n",
        "print(\"Available columns in dataset:\", df.columns)\n",
        "\n",
        "# üîµ Please replace 'your_column_name' below with the actual column name from the output above\n",
        "# üüß For example, if the column is 'population' or 'value', use that instead of 'your_column_name'\n",
        "data = df['your_column_name'].values.astype(np.float32)  # üîÅ Replace this\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "xzbYXVZmnQ2A",
        "outputId": "789a365c-65e2-4404-dec2-1022ec1b8932"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns in dataset: Index(['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4',\n",
            "       'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n",
            "       'Dependency Ratio', 'Unnamed: 11', 'Unnamed: 12', 'Median Age',\n",
            "       'Unnamed: 14', 'Unnamed: 15'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'your_column_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'your_column_name'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-75a18285ece7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# üîµ Please replace 'your_column_name' below with the actual column name from the output above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# üüß For example, if the column is 'population' or 'value', use that instead of 'your_column_name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'your_column_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# üîÅ Replace this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'your_column_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üü® Step 4: Create rolling windows of inputs and corresponding multi-step outputs\n",
        "# üîµ (a#) We need to create input-output pairs for training the model\n",
        "# üîµ (b#) Using sliding windows, we generate training samples from historical data\n",
        "# üîµ (c#) Each input is a sequence of time steps (e.g., 20), and each output is a future value at multiple horizons\n",
        "# üîµ (d#) This allows the model to learn from past data and make multi-horizon predictions\n",
        "\n",
        "input_window = 20\n",
        "forecast_steps = [1, 5, 10, 20]  # Forecast horizons: t+1, t+5, t+10, t+20\n",
        "\n",
        "X_multi = []\n",
        "Y_multi = []\n",
        "\n",
        "for i in range(len(data) - input_window - max(forecast_steps)):\n",
        "    x = data[i : i + input_window]\n",
        "    y = [data[i + input_window + h - 1] for h in forecast_steps]\n",
        "    X_multi.append(x)\n",
        "    Y_multi.append(y)\n",
        "\n",
        "X_multi = np.array(X_multi).reshape(-1, input_window, 1)\n",
        "Y_multi = np.array(Y_multi)\n",
        "\n",
        "# üü™ Step 5: Print and verify the shape of processed data\n",
        "# üîµ (a#) This confirms the model input and output dimensions are correctly prepared\n",
        "# üîµ (b#) Avoids runtime errors when passed into the model\n",
        "# üîµ (c#) `.shape` shows the dimensions: (samples, time steps, features)\n",
        "# üîµ (d#) Now the model can process sequences and predict multi-horizon values\n",
        "\n",
        "print(f\"‚úÖ Shape of X_multi: {X_multi.shape}\")  # Expected: (samples, 20, 1)\n",
        "print(f\"‚úÖ Shape of Y_multi: {Y_multi.shape}\")  # Expected: (samples, 4)\n",
        "\n",
        "# ‚úÖ Safe to access shape[1] now\n",
        "forecast_horizons = Y_multi.shape[1]\n"
      ],
      "metadata": {
        "id": "yFkfvSbqnUmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assume df_resampled is your cleaned and resampled dataframe\n",
        "# Define the number of time steps for the input sequence and the number of steps for the forecast horizon\n",
        "sequence_length = 10  # Number of time steps for each input sequence\n",
        "forecast_horizons = 5  # Forecast for the next 5 time steps\n",
        "\n",
        "# Initialize X_multi (input) and Y_multi (output/forecast) arrays\n",
        "X_multi = []\n",
        "Y_multi = []\n",
        "\n",
        "# Loop through the dataset and create windows\n",
        "for i in range(len(df_resampled) - sequence_length - forecast_horizons + 1):\n",
        "    # Input sequence (X) - previous 'sequence_length' time steps\n",
        "    X = df_resampled.iloc[i:i+sequence_length].values\n",
        "\n",
        "    # Output sequence (Y) - forecast for the next 'forecast_horizons' time steps\n",
        "    Y = df_resampled.iloc[i+sequence_length:i+sequence_length+forecast_horizons].values\n",
        "\n",
        "    X_multi.append(X)\n",
        "    Y_multi.append(Y)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_multi = np.array(X_multi)\n",
        "Y_multi = np.array(Y_multi)\n",
        "\n",
        "# Check the shapes to confirm data preparation\n",
        "print(f\"Shape of X_multi: {X_multi.shape}\")\n",
        "print(f\"Shape of Y_multi: {Y_multi.shape}\")\n"
      ],
      "metadata": {
        "id": "4wijMBAhWwtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your data (convert to PyTorch tensors)\n",
        "X_train = torch.tensor(X_multi, dtype=torch.float32)\n",
        "Y_train = torch.tensor(Y_multi, dtype=torch.float32)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train)  # Predicted horizons: (batch_size, forecast_horizons)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_fn(y_pred, Y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "rKDdO7-tUtTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Example: Forecast the next 20 years based on the latest input window\n",
        "    x_input = torch.tensor(X_multi[-1:], dtype=torch.float32)  # Last input\n",
        "    forecast = model(x_input)\n",
        "\n",
        "    # Print forecasted values for t+1, t+5, t+10, t+20\n",
        "    print(\"Forecasted values:\", forecast)\n"
      ],
      "metadata": {
        "id": "-rNgXfPSUv86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/time_series_transformer.pth')\n",
        "\n",
        "# Load the model for future predictions\n",
        "model = TimeSeriesTransformer(input_size, hidden_size, num_heads, num_layers, forecast_horizons)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/time_series_transformer.pth'))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "HKViULOwUzLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the previously trained model from Google Drive\n",
        "model = TimeSeriesTransformer(input_size, hidden_size, num_heads, num_layers, forecast_horizons)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/time_series_transformer.pth'))\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "\n",
        "# Assume df_resampled is your preprocessed data\n",
        "# If df_resampled is a Pandas DataFrame, ensure it's in the appropriate format\n",
        "# Example: df_resampled has shape (num_samples, num_features)\n"
      ],
      "metadata": {
        "id": "jxsq4LjYYIv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) In this step, we load the trained model to continue training or evaluate it on new data.\n",
        "# b) This code uses the 'load_state_dict' function to restore the model's parameters from a saved file.\n",
        "# c) The 'eval' function sets the model to evaluation mode, which ensures that operations like dropout are disabled.\n",
        "# d) The purpose of this code is to load the pre-trained model and prepare it for inference or further training.\n"
      ],
      "metadata": {
        "id": "712wKKvFYThD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Temporal Discounting Loss function\n",
        "class TemporalDiscountingLoss(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super(TemporalDiscountingLoss, self).__init__()\n",
        "        self.weights = torch.tensor(weights, dtype=torch.float32)  # Decaying weights for different forecast horizons\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # predictions: (batch_size, forecast_horizons)\n",
        "        # targets: (batch_size, forecast_horizons)\n",
        "\n",
        "        # Compute the individual losses for each forecast horizon (e.g., t+1, t+5, t+10, t+20)\n",
        "        losses = (predictions - targets) ** 2  # Mean squared error for each horizon\n",
        "\n",
        "        # Apply the weights to each loss\n",
        "        weighted_losses = losses * self.weights\n",
        "\n",
        "        # Return the sum of weighted losses\n",
        "        return weighted_losses.sum()\n"
      ],
      "metadata": {
        "id": "WD6WsYx8YWNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) We are using Temporal Discounting Loss to ensure the model prioritizes long-term forecasting accuracy.\n",
        "#    By weighting the losses differently for different forecast horizons, we can emphasize the importance of predicting long-term values.\n",
        "# b) The TemporalDiscountingLoss class calculates the squared error loss for each forecast horizon and then applies the weight vector to each.\n",
        "#    This ensures that we penalize the model more for errors in long-term predictions (e.g., t+10, t+20).\n",
        "# c) 'weights' is the vector that defines the temporal discounting at each forecast horizon, and 'losses' are the squared differences between predictions and targets.\n",
        "# d) This operation will return the sum of weighted losses, which is used to guide the model during training towards better long-term forecasting.\n"
      ],
      "metadata": {
        "id": "1SQYo0hZYaXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function with the weights for temporal discounting\n",
        "loss_weights = [0.1, 0.2, 0.4, 0.8]  # Example weights for t+1, t+5, t+10, t+20\n",
        "criterion = TemporalDiscountingLoss(loss_weights)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Assuming X_train and Y_train are your training data (already windowed)\n",
        "    # Convert them to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_multi, dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_multi, dtype=torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model(X_train_tensor)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(predictions, Y_train_tensor)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss for each epoch\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "gQEj2Q4UYdbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) In this code, we are training the model with the Temporal Discounting Loss to focus on long-term forecasting.\n",
        "#    By using weighted losses, we enforce that the model learns better long-term predictions (e.g., t+10, t+20).\n",
        "# b) The optimizer (Adam) is used to adjust the model parameters based on the gradients of the loss function.\n",
        "#    We use 'loss.backward()' to compute these gradients and 'optimizer.step()' to update the parameters.\n",
        "# c) 'X_train_tensor' and 'Y_train_tensor' are the training data (input and target). The 'predictions' are the model's outputs.\n",
        "# d) The result of this operation is a model that learns with temporal discounting loss, improving long-term forecasting accuracy.\n"
      ],
      "metadata": {
        "id": "EqIDvfuWYgZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on a test set (using the same windowing approach for X_test and Y_test)\n",
        "model.eval()  # Switch to evaluation mode\n",
        "\n",
        "# Assuming X_test and Y_test are your test data (already windowed)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "# Get the predictions\n",
        "with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "    predictions = model(X_test_tensor)\n",
        "\n",
        "# Compute the loss on the test set\n",
        "test_loss = criterion(predictions, Y_test_tensor)\n",
        "print(f\"Test Loss: {test_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "0yPKb-ZuYjN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) After training, we need to evaluate the model to see how well it generalizes to unseen data (test set).\n",
        "#    The model is switched to evaluation mode using 'model.eval()', which disables certain layers like dropout.\n",
        "# b) The 'torch.no_grad()' context ensures that gradients are not computed during evaluation, saving memory and computation.\n",
        "#    We then compute the loss for the test set using the Temporal Discounting Loss function.\n",
        "# c) 'X_test_tensor' and 'Y_test_tensor' are the input and target data for the test set. 'predictions' are the model's outputs for this set.\n",
        "# d) The result is a 'test_loss' that gives an indication of how well the model performs on unseen data using the weighted loss strategy.\n"
      ],
      "metadata": {
        "id": "SM4QG1zFYmVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert predictions and true values to numpy arrays for plotting\n",
        "predictions_np = predictions.numpy()\n",
        "Y_test_np = Y_test_tensor.numpy()\n",
        "\n",
        "# Plot the results for a few forecast horizons (e.g., t+1, t+5, t+10)\n",
        "horizons = ['t+1', 't+5', 't+10', 't+20']\n",
        "\n",
        "# Plot predictions vs actual for each horizon\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, horizon in enumerate(horizons):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(Y_test_np[:, i], label='True Values')\n",
        "    plt.plot(predictions_np[:, i], label='Predictions')\n",
        "    plt.title(f\"Forecast Horizon: {horizon}\")\n",
        "    plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gnb1E-yuYpRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) We use visualization to compare the true values and predictions from the model, helping us understand its performance at different forecast horizons.\n",
        "# b) The plot allows us to visually assess how well the model is performing at various forecast horizons (e.g., t+1, t+5, t+10, t+20).\n",
        "# c) 'plt.plot()' creates a line plot for true vs. predicted values. 'horizons' is a list of forecast time steps (e.g., t+1, t+5, t+10, t+20).\n",
        "# d) The result is a set of plots showing the predictions compared to the actual values for each forecast horizon.\n"
      ],
      "metadata": {
        "id": "qUWBR3pnYua2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/time_series_transformer_trained.pth')\n",
        "\n",
        "# To load the model later for prediction\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/time_series_transformer_trained.pth'))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "f2PHdceKYv4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) After training and evaluating the model, we save the model to Google Drive for future use.\n",
        "#    Saving the model allows us to avoid retraining it each time we want to use it for predictions.\n",
        "# b) 'torch.save()' is used to store the model's parameters, while 'load_state_dict()' is used to restore these parameters later.\n",
        "# c) 'state_dict' refers to the dictionary containing the model's parameters (weights). The model is restored by loading this state.\n",
        "# d) This ensures that the model is available for future use, either for making predictions or fine-tuning it further.\n"
      ],
      "metadata": {
        "id": "yGo14gk4Y2Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a function to compute the penalty for violating monotonicity constraint\n",
        "def monotonicity_penalty(predictions):\n",
        "    \"\"\"\n",
        "    This function computes the penalty for violating the monotonicity constraint.\n",
        "    The monotonicity constraint enforces that the uncertainty (or variance) increases with time.\n",
        "    \"\"\"\n",
        "    # Calculate the difference between consecutive predictions (for each forecast horizon)\n",
        "    # The assumption here is that the forecasted uncertainty should increase with time\n",
        "    diff = torch.diff(predictions, dim=1)\n",
        "\n",
        "    # Penalize negative differences, as we want the variance to increase (positive diff)\n",
        "    penalty = torch.sum(torch.relu(-diff))  # Only penalize if the difference is negative\n",
        "\n",
        "    return penalty\n",
        "\n",
        "# Example: assuming predictions from the model\n",
        "# predictions: (batch_size, forecast_horizons)\n",
        "predictions = torch.tensor([[0.1, 0.15, 0.2, 0.25],  # Sample prediction for one batch\n",
        "                            [0.2, 0.3, 0.4, 0.5]])  # Sample prediction for another batch\n",
        "\n",
        "# Compute the monotonicity penalty for the predictions\n",
        "penalty = monotonicity_penalty(predictions)\n",
        "print(f\"Monotonicity penalty: {penalty.item()}\")\n"
      ],
      "metadata": {
        "id": "1fZtS5lRZw8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) We are using the monotonicity penalty to enforce that the uncertainty (or variance) increases over time. This helps in making long-term forecasts more reliable and consistent.\n",
        "# b) The function 'monotonicity_penalty' calculates the difference between consecutive time steps (horizons) in the predictions.\n",
        "#    It then penalizes negative differences using the 'torch.relu' function, which sets any negative differences to zero, ensuring that the penalty is applied when the model's uncertainty decreases over time.\n",
        "# c) 'torch.diff(predictions, dim=1)' computes the difference between consecutive forecast horizons. The 'torch.relu' function is used to apply the penalty only when the difference is negative (i.e., the uncertainty decreases).\n",
        "# d) The result of this operation is a penalty value that will be added to the model's total loss, encouraging the model to increase uncertainty over time.\n"
      ],
      "metadata": {
        "id": "oCvSASnxabDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the loss function to include the monotonicity penalty\n",
        "class TemporalDiscountingLossWithMonotonicity(nn.Module):\n",
        "    def __init__(self, weights, monotonicity_weight=0.1):\n",
        "        super(TemporalDiscountingLossWithMonotonicity, self).__init__()\n",
        "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
        "        self.monotonicity_weight = monotonicity_weight  # Weight of the monotonicity penalty in the total loss\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Compute the standard temporal discounting loss\n",
        "        losses = (predictions - targets) ** 2  # Mean squared error for each horizon\n",
        "        weighted_losses = losses * self.weights\n",
        "\n",
        "        # Compute the monotonicity penalty\n",
        "        monotonicity_penalty_value = monotonicity_penalty(predictions)\n",
        "\n",
        "        # Return the total loss, which is the sum of weighted losses and the monotonicity penalty\n",
        "        total_loss = weighted_losses.sum() + self.monotonicity_weight * monotonicity_penalty_value\n",
        "        return total_loss\n",
        "\n",
        "# Example usage in training loop\n",
        "# Assuming X_train and Y_train are our training data (already windowed)\n",
        "X_train_tensor = torch.tensor(X_multi, dtype=torch.float32)\n",
        "Y_train_tensor = torch.tensor(Y_multi, dtype=torch.float32)\n",
        "\n",
        "# Define the loss function with monotonicity penalty\n",
        "loss_weights = [0.1, 0.2, 0.4, 0.8]  # Example weights for t+1, t+5, t+10, t+20\n",
        "criterion = TemporalDiscountingLossWithMonotonicity(loss_weights)\n",
        "\n",
        "# Example model training loop with monotonicity constraints\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model(X_train_tensor)\n",
        "\n",
        "    # Compute the loss with the monotonicity penalty\n",
        "    loss = criterion(predictions, Y_train_tensor)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "NPrISossaeI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) By including the monotonicity penalty in the loss function, we are directly enforcing the rule that uncertainty should increase over time.\n",
        "# b) The modified loss function, 'TemporalDiscountingLossWithMonotonicity', combines the traditional temporal discounting loss with the monotonicity penalty.\n",
        "#    This ensures that the model is penalized not only for making incorrect predictions but also for violating the monotonicity constraint.\n",
        "# c) The 'monotonicity_weight' controls how much importance we give to the monotonicity penalty compared to the regular forecasting loss. A higher value would place more emphasis on monotonicity.\n",
        "# d) This operation results in a total loss that guides the model to follow both the temporal discounting and the monotonicity rules, improving the long-term forecasting behavior.\n"
      ],
      "metadata": {
        "id": "5o4Z5Sskagpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on a test set\n",
        "model.eval()\n",
        "\n",
        "# Assuming X_test and Y_test are your test data (already windowed)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "# Get the predictions from the model\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test_tensor)\n",
        "\n",
        "# Compute the test loss including the monotonicity penalty\n",
        "test_loss = criterion(predictions, Y_test_tensor)\n",
        "print(f\"Test Loss (with Monotonicity Penalty): {test_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "30xcvjP2ajhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) In this step, we evaluate the model on a test set to understand its generalization ability with the monotonicity constraints applied.\n",
        "# b) By calling 'model.eval()', we ensure the model is in evaluation mode, which disables dropout and other training-specific behaviors.\n",
        "#    We then compute the loss for the test set by using the same temporal discounting and monotonicity penalty.\n",
        "# c) 'X_test_tensor' and 'Y_test_tensor' are the test data in tensor format. The 'predictions' are the outputs of the model when evaluated on this data.\n",
        "# d) This operation will provide the test loss, which includes both the temporal discounting loss and the penalty for monotonicity violations. This helps in determining how well the model adheres to the monotonicity rule on unseen data.\n"
      ],
      "metadata": {
        "id": "IcGTbTlEamAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert predictions and true values to numpy arrays for plotting\n",
        "predictions_np = predictions.numpy()\n",
        "Y_test_np = Y_test_tensor.numpy()\n",
        "\n",
        "# Plot predictions vs actual for a few forecast horizons (e.g., t+1, t+5, t+10)\n",
        "horizons = ['t+1', 't+5', 't+10', 't+20']\n",
        "\n",
        "# Plot predictions vs actual for each horizon\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, horizon in enumerate(horizons):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(Y_test_np[:, i], label='True Values')\n",
        "    plt.plot(predictions_np[:, i], label='Predictions')\n",
        "    plt.title(f\"Forecast Horizon: {horizon}\")\n",
        "    plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P0up99sdapA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Visualization helps in comparing the model's predictions against the true values to understand its performance, particularly at different forecast horizons.\n",
        "# b) By plotting the true and predicted values for each forecast horizon, we can visually inspect how well the model is adhering to the monotonicity constraints.\n",
        "# c) 'plt.plot()' creates a line plot for true vs. predicted values. 'horizons' is a list of forecast time steps (e.g., t+1, t+5, t+10, t+20).\n",
        "# d) The result will be a set of plots showing the predictions compared to the actual values for each forecast horizon, which will help us understand how well the model is performing under the monotonicity constraints.\n"
      ],
      "metadata": {
        "id": "3-7XJK07arkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute the Fourier Transform of a time series\n",
        "def compute_fft(x):\n",
        "    \"\"\"\n",
        "    Compute the Fast Fourier Transform (FFT) of the input time series.\n",
        "    FFT helps in analyzing the frequency components of the signal.\n",
        "    \"\"\"\n",
        "    # Apply FFT on the input tensor (time series)\n",
        "    fft_result = torch.fft.fft(x)\n",
        "\n",
        "    # Get the magnitude of the complex FFT result (frequency domain representation)\n",
        "    fft_magnitude = torch.abs(fft_result)\n",
        "\n",
        "    return fft_magnitude\n",
        "\n",
        "# Define a function to compute the frequency-aware loss\n",
        "def frequency_aware_loss(predictions, targets):\n",
        "    \"\"\"\n",
        "    Frequency-Aware Loss that compares the predicted signal and the target signal in the frequency domain.\n",
        "    \"\"\"\n",
        "    # Apply FFT on the predictions and targets\n",
        "    prediction_fft = compute_fft(predictions)\n",
        "    target_fft = compute_fft(targets)\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) in the frequency domain\n",
        "    mse_freq = torch.mean((prediction_fft - target_fft) ** 2)\n",
        "\n",
        "    return mse_freq\n"
      ],
      "metadata": {
        "id": "0zw_MfyzauZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) We are using FFT to transform the time series data into the frequency domain, allowing us to capture long-term periodicities while avoiding short-term noise.\n",
        "# b) The function 'compute_fft' performs the Fast Fourier Transform on the time series data, converting it into the frequency domain.\n",
        "#    The 'frequency_aware_loss' function then compares the magnitude of the FFT of the predicted signal with the true signal, ensuring that the model adheres to the frequency patterns.\n",
        "# c) 'torch.fft.fft' computes the FFT of a tensor. The result is a complex number array, from which we compute the magnitude using 'torch.abs' to get the frequency components. The MSE is computed in the frequency domain to capture periodicity differences.\n",
        "# d) This operation helps the model focus on long-term periodic patterns in the data by penalizing discrepancies in the frequency domain, thereby reducing the risk of overfitting short-term noise.\n"
      ],
      "metadata": {
        "id": "gOvifDhSbWp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example model and training loop with Frequency-Aware Loss\n",
        "class TimeSeriesTransformerWithFreqLoss(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_heads, num_layers, forecast_horizons, dropout=0.1):\n",
        "        super(TimeSeriesTransformerWithFreqLoss, self).__init__()\n",
        "\n",
        "        # Define embedding layer\n",
        "        self.embedding = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Transformer Encoder and Decoder\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_size, forecast_horizons)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, input_size)\n",
        "        x = self.embedding(x)  # (batch_size, sequence_length, hidden_size)\n",
        "        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, hidden_size)\n",
        "\n",
        "        # Transformer expects the target sequence as well (for now, using x as both)\n",
        "        transformer_output = self.transformer(x, x)  # (sequence_length, batch_size, hidden_size)\n",
        "\n",
        "        # Get the last time step's output for forecasting\n",
        "        forecast = transformer_output[-1, :, :]  # (batch_size, hidden_size)\n",
        "\n",
        "        # Final output layer to predict multiple horizons\n",
        "        output = self.output_layer(forecast)  # (batch_size, forecast_horizons)\n",
        "        return output\n",
        "\n",
        "# Example model parameters (adjust based on your data)\n",
        "input_size = X_multi.shape[2]  # Number of features in input\n",
        "hidden_size = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "forecast_horizons = Y_multi.shape[1]  # forecast horizons\n",
        "\n",
        "# Instantiate the model\n",
        "model_with_freq_loss = TimeSeriesTransformerWithFreqLoss(input_size, hidden_size, num_heads, num_layers, forecast_horizons)\n",
        "\n",
        "# Define the optimizer and the frequency-aware loss criterion\n",
        "optimizer = optim.Adam(model_with_freq_loss.parameters(), lr=0.001)\n",
        "\n",
        "# Example training loop with frequency-aware loss\n",
        "for epoch in range(50):\n",
        "    model_with_freq_loss.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model_with_freq_loss(X_train_tensor)\n",
        "\n",
        "    # Compute the frequency-aware loss\n",
        "    loss = frequency_aware_loss(predictions, Y_train_tensor)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "gp5k6DlLbZuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) We are integrating the Frequency-Aware Loss into the training loop to ensure that the model captures long-term periodic patterns while avoiding overfitting to short-term noise.\n",
        "# b) The model 'TimeSeriesTransformerWithFreqLoss' is trained using the same architecture as before, but with an additional frequency-aware loss term.\n",
        "#    The 'frequency_aware_loss' function is called during training, and it computes the discrepancy between the predicted and true signals in the frequency domain, encouraging the model to focus on periodicity rather than noise.\n",
        "# c) The optimizer (Adam) is used to minimize the total loss, which now includes both the standard loss and the frequency-aware loss.\n",
        "# d) This operation ensures that the model will be regularized with respect to frequency patterns, improving its ability to generalize over long-term periodicities.\n"
      ],
      "metadata": {
        "id": "hxYFdxbobeOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on a test set with frequency-aware loss\n",
        "model_with_freq_loss.eval()\n",
        "\n",
        "# Assuming X_test and Y_test are your test data (already windowed)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "# Get the predictions from the model\n",
        "with torch.no_grad():\n",
        "    predictions = model_with_freq_loss(X_test_tensor)\n",
        "\n",
        "# Compute the test loss including the frequency-aware penalty\n",
        "test_loss = frequency_aware_loss(predictions, Y_test_tensor)\n",
        "print(f\"Test Loss (with Frequency-Aware Loss): {test_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "YG9nHWvMbin9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Evaluating the model with frequency-aware loss ensures that the model is generalizing well to unseen data while capturing long-term periodicities.\n",
        "# b) By calling 'model.eval()', we disable training-specific behaviors (like dropout) to evaluate the model's performance on the test set.\n",
        "#    The frequency-aware loss is computed on the test set, ensuring that the model has learned to focus on periodicity in the predictions.\n",
        "# c) The 'predictions' are the model outputs, and 'test_loss' is the loss calculated using the frequency-aware criterion, which penalizes discrepancies in the frequency domain.\n",
        "# d) This operation provides an evaluation of how well the model has learned to capture periodic patterns and avoid overfitting to noise in unseen data.\n"
      ],
      "metadata": {
        "id": "lpzQVEhZbwkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert predictions and true values to numpy arrays for plotting\n",
        "predictions_np = predictions.numpy()\n",
        "Y_test_np = Y_test_tensor.numpy()\n",
        "\n",
        "# Plot predictions vs actual for a few forecast horizons (e.g., t+1, t+5, t+10)\n",
        "horizons = ['t+1', 't+5', 't+10', 't+20']\n",
        "\n",
        "# Plot predictions vs actual for each horizon\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, horizon in enumerate(horizons):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(Y_test_np[:, i], label='True Values')\n",
        "    plt.plot(predictions_np[:, i], label='Predictions')\n",
        "    plt.title(f\"Forecast Horizon: {horizon}\")\n",
        "    plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XXyMGltEbyNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Visualization helps in comparing the model's predictions against the true values and analyzing how well it captures the long-term periodicities.\n",
        "# b) By plotting the true and predicted values for each forecast horizon, we can visually inspect how the model is performing in both time and frequency domains.\n",
        "# c) 'plt.plot()' creates line plots for true vs. predicted values. The 'horizons' list corresponds to the forecasted time steps (e.g., t+1, t+5, t+10, t+20).\n",
        "# d) The result is a set of plots that will show the predictions in comparison with actual values, helping to identify if the model is adhering to periodicity and avoiding short-term noise.\n"
      ],
      "metadata": {
        "id": "mvSp8ENMbyR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a function to compute the consistency loss\n",
        "def consistency_loss(predictions):\n",
        "    \"\"\"\n",
        "    Ensures that predictions across consecutive time steps (e.g., t+5, t+6, t+7) are consistent.\n",
        "    Adds a penalty if the change between consecutive time steps is too large.\n",
        "    \"\"\"\n",
        "    # Calculate the absolute differences between consecutive time steps\n",
        "    diff = torch.abs(predictions[:, 1:] - predictions[:, :-1])  # |y_{t+5} - y_{t+6}|\n",
        "\n",
        "    # Calculate the mean absolute difference (MAD) for consistency penalty\n",
        "    consistency_penalty = torch.mean(diff)\n",
        "\n",
        "    return consistency_penalty\n"
      ],
      "metadata": {
        "id": "bUn2Fz5qcEAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) We are using the Consistency Loss to penalize large jumps or oscillations between consecutive time steps (like t+5, t+6, t+7).\n",
        "# b) The function 'consistency_loss' computes the absolute differences between consecutive predictions in the forecast horizon. This difference is then averaged to calculate the consistency penalty.\n",
        "# c) The input 'predictions' is a tensor of shape (batch_size, forecast_horizons), where each column corresponds to a time step (e.g., t+1, t+5, t+6). The difference between consecutive predictions is computed using 'torch.abs' to get the absolute value of the change.\n",
        "# d) This penalty ensures that the model predictions are smooth and consistent, preventing large, unrealistic jumps between neighboring forecast horizons (e.g., t+5, t+6, t+7).\n"
      ],
      "metadata": {
        "id": "Se1xjzAmc9yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model class with consistency constraint added\n",
        "class TimeSeriesTransformerWithConsistency(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_heads, num_layers, forecast_horizons, dropout=0.1):\n",
        "        super(TimeSeriesTransformerWithConsistency, self).__init__()\n",
        "\n",
        "        # Define embedding layer\n",
        "        self.embedding = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Transformer Encoder and Decoder\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_size, forecast_horizons)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, input_size)\n",
        "        x = self.embedding(x)  # (batch_size, sequence_length, hidden_size)\n",
        "        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, hidden_size)\n",
        "\n",
        "        # Transformer expects the target sequence as well (for now, using x as both)\n",
        "        transformer_output = self.transformer(x, x)  # (sequence_length, batch_size, hidden_size)\n",
        "\n",
        "        # Get the last time step's output for forecasting\n",
        "        forecast = transformer_output[-1, :, :]  # (batch_size, hidden_size)\n",
        "\n",
        "        # Final output layer to predict multiple horizons\n",
        "        output = self.output_layer(forecast)  # (batch_size, forecast_horizons)\n",
        "        return output\n",
        "\n",
        "# Example model parameters (adjust based on your data)\n",
        "input_size = X_multi.shape[2]  # Number of features in input\n",
        "hidden_size = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "forecast_horizons = Y_multi.shape[1]  # forecast horizons\n",
        "\n",
        "# Instantiate the model\n",
        "model_with_consistency = TimeSeriesTransformerWithConsistency(input_size, hidden_size, num_heads, num_layers, forecast_horizons)\n",
        "\n",
        "# Define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model_with_consistency.parameters(), lr=0.001)\n",
        "\n",
        "# Example training loop with consistency loss\n",
        "for epoch in range(50):\n",
        "    model_with_consistency.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model_with_consistency(X_train_tensor)\n",
        "\n",
        "    # Compute the consistency loss\n",
        "    loss = consistency_loss(predictions)  # Consistency penalty\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Consistency Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "cD1cQIH-dBuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) The model is trained to minimize both the prediction error and the consistency loss, ensuring smooth transitions between consecutive forecast horizons.\n",
        "# b) The 'TimeSeriesTransformerWithConsistency' class is designed with the same architecture as before, but with an added consistency loss term.\n",
        "#    The 'consistency_loss' function is called during training to penalize large jumps or oscillations between consecutive time steps, such as t+5, t+6, t+7.\n",
        "# c) The optimizer (Adam) is used to minimize the total loss, which includes both the prediction loss and the consistency penalty.\n",
        "# d) This operation ensures that the model does not produce erratic or unrealistic predictions between consecutive time steps and the transitions between forecast horizons remain smooth.\n"
      ],
      "metadata": {
        "id": "aFGqHGnGdObm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on a test set with consistency loss\n",
        "model_with_consistency.eval()\n",
        "\n",
        "# Assuming X_test and Y_test are your test data (already windowed)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "# Get the predictions from the model\n",
        "with torch.no_grad():\n",
        "    predictions = model_with_consistency(X_test_tensor)\n",
        "\n",
        "# Compute the test consistency loss\n",
        "test_loss = consistency_loss(predictions)\n",
        "print(f\"Test Loss (with Consistency Loss): {test_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "pbPrDEqkdR_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Evaluating the model with consistency loss ensures that the model is making smooth and consistent predictions across forecast horizons.\n",
        "# b) By calling 'model.eval()', we disable training-specific behaviors (like dropout) and evaluate the model on the test data.\n",
        "#    The 'consistency_loss' function is then applied to the predictions on the test set, penalizing any large jumps between consecutive time steps.\n",
        "# c) The 'predictions' are the model's output, and 'test_loss' is the calculated consistency loss for the test set.\n",
        "# d) This operation provides an evaluation of the model's ability to maintain smooth transitions and consistency in predictions across neighboring forecast horizons.\n"
      ],
      "metadata": {
        "id": "VRv3xjImdWns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert predictions and true values to numpy arrays for plotting\n",
        "predictions_np = predictions.numpy()\n",
        "Y_test_np = Y_test_tensor.numpy()\n",
        "\n",
        "# Plot predictions vs actual for a few forecast horizons (e.g., t+5, t+6, t+7)\n",
        "horizons = ['t+5', 't+6', 't+7']\n",
        "\n",
        "# Plot predictions vs actual for each horizon\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, horizon in enumerate(horizons):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(Y_test_np[:, i], label='True Values')\n",
        "    plt.plot(predictions_np[:, i], label='Predictions')\n",
        "    plt.title(f\"Forecast Horizon: {horizon}\")\n",
        "    plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NDb1sI3idYIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Visualization helps in comparing the model's predictions against the true values and analyzing the consistency of the predictions across time steps.\n",
        "# b) By plotting the true and predicted values for each forecast horizon, we can visually inspect how consistent the model is at making predictions for nearby horizons.\n",
        "# c) 'plt.plot()' creates line plots for true vs. predicted values. The 'horizons' list corresponds to the forecasted time steps (e.g., t+5, t+6, t+7).\n",
        "# d) The result is a set of plots that show how consistent the predictions are for consecutive time steps, helping us evaluate the smoothness of the forecast and whether large jumps are avoided.\n"
      ],
      "metadata": {
        "id": "VDf1-ZPGdb4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a function to compute the consistency loss\n",
        "def consistency_loss(predictions):\n",
        "    \"\"\"\n",
        "    Ensures that predictions across consecutive time steps (e.g., t+5, t+6, t+7) are consistent.\n",
        "    Adds a penalty if the change between consecutive time steps is too large.\n",
        "    \"\"\"\n",
        "    # Calculate the absolute differences between consecutive time steps\n",
        "    diff = torch.abs(predictions[:, 1:] - predictions[:, :-1])  # |y_{t+5} - y_{t+6}|\n",
        "\n",
        "    # Calculate the mean absolute difference (MAD) for consistency penalty\n",
        "    consistency_penalty = torch.mean(diff)\n",
        "\n",
        "    return consistency_penalty\n"
      ],
      "metadata": {
        "id": "S0_hLK50gPdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) We are using the Consistency Loss to penalize large jumps or oscillations between consecutive time steps (like t+5, t+6, t+7).\n",
        "# b) The function 'consistency_loss' computes the absolute differences between consecutive predictions in the forecast horizon. This difference is then averaged to calculate the consistency penalty.\n",
        "# c) The input 'predictions' is a tensor of shape (batch_size, forecast_horizons), where each column corresponds to a time step (e.g., t+1, t+5, t+6). The difference between consecutive predictions is computed using 'torch.abs' to get the absolute value of the change.\n",
        "# d) This penalty ensures that the model predictions are smooth and consistent, preventing large, unrealistic jumps between neighboring forecast horizons (e.g., t+5, t+6, t+7).\n"
      ],
      "metadata": {
        "id": "22URD2uShbhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model class with consistency constraint added\n",
        "class TimeSeriesTransformerWithConsistency(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_heads, num_layers, forecast_horizons, dropout=0.1):\n",
        "        super(TimeSeriesTransformerWithConsistency, self).__init__()\n",
        "\n",
        "        # Define embedding layer\n",
        "        self.embedding = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Transformer Encoder and Decoder\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_size, forecast_horizons)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, input_size)\n",
        "        x = self.embedding(x)  # (batch_size, sequence_length, hidden_size)\n",
        "        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, hidden_size)\n",
        "\n",
        "        # Transformer expects the target sequence as well (for now, using x as both)\n",
        "        transformer_output = self.transformer(x, x)  # (sequence_length, batch_size, hidden_size)\n",
        "\n",
        "        # Get the last time step's output for forecasting\n",
        "        forecast = transformer_output[-1, :, :]  # (batch_size, hidden_size)\n",
        "\n",
        "        # Final output layer to predict multiple horizons\n",
        "        output = self.output_layer(forecast)  # (batch_size, forecast_horizons)\n",
        "        return output\n",
        "\n",
        "# Example model parameters (adjust based on your data)\n",
        "input_size = X_multi.shape[2]  # Number of features in input\n",
        "hidden_size = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "forecast_horizons = Y_multi.shape[1]  # forecast horizons\n",
        "\n",
        "# Instantiate the model\n",
        "model_with_consistency = TimeSeriesTransformerWithConsistency(input_size, hidden_size, num_heads, num_layers, forecast_horizons)\n",
        "\n",
        "# Define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model_with_consistency.parameters(), lr=0.001)\n",
        "\n",
        "# Example training loop with consistency loss\n",
        "for epoch in range(50):\n",
        "    model_with_consistency.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model_with_consistency(X_train_tensor)\n",
        "\n",
        "    # Compute the consistency loss\n",
        "    loss = consistency_loss(predictions)  # Consistency penalty\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Consistency Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "lbTO6B_QhdpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) The model is trained to minimize both the prediction error and the consistency loss, ensuring smooth transitions between consecutive forecast horizons.\n",
        "# b) The 'TimeSeriesTransformerWithConsistency' class is designed with the same architecture as before, but with an added consistency loss term.\n",
        "#    The 'consistency_loss' function is called during training to penalize large jumps or oscillations between consecutive time steps, such as t+5, t+6, t+7.\n",
        "# c) The optimizer (Adam) is used to minimize the total loss, which includes both the prediction loss and the consistency penalty.\n",
        "# d) This operation ensures that the model does not produce erratic or unrealistic predictions between consecutive time steps and the transitions between forecast horizons remain smooth.\n"
      ],
      "metadata": {
        "id": "4jjjU67shgFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on a test set with consistency loss\n",
        "model_with_consistency.eval()\n",
        "\n",
        "# Assuming X_test and Y_test are your test data (already windowed)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "# Get the predictions from the model\n",
        "with torch.no_grad():\n",
        "    predictions = model_with_consistency(X_test_tensor)\n",
        "\n",
        "# Compute the test consistency loss\n",
        "test_loss = consistency_loss(predictions)\n",
        "print(f\"Test Loss (with Consistency Loss): {test_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "UtVndZkIhkBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Evaluating the model with consistency loss ensures that the model is making smooth and consistent predictions across forecast horizons.\n",
        "# b) By calling 'model.eval()', we disable training-specific behaviors (like dropout) and evaluate the model on the test data.\n",
        "#    The 'consistency_loss' function is then applied to the predictions on the test set, penalizing any large jumps between consecutive time steps.\n",
        "# c) The 'predictions' are the model's output, and 'test_loss' is the calculated consistency loss for the test set.\n",
        "# d) This operation provides an evaluation of the model's ability to maintain smooth transitions and consistency in predictions across neighboring forecast horizons.\n"
      ],
      "metadata": {
        "id": "Yu2CYelnhl-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert predictions and true values to numpy arrays for plotting\n",
        "predictions_np = predictions.numpy()\n",
        "Y_test_np = Y_test_tensor.numpy()\n",
        "\n",
        "# Plot predictions vs actual for a few forecast horizons (e.g., t+5, t+6, t+7)\n",
        "horizons = ['t+5', 't+6', 't+7']\n",
        "\n",
        "# Plot predictions vs actual for each horizon\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, horizon in enumerate(horizons):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(Y_test_np[:, i], label='True Values')\n",
        "    plt.plot(predictions_np[:, i], label='Predictions')\n",
        "    plt.title(f\"Forecast Horizon: {horizon}\")\n",
        "    plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8NKZWCjRhp4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Visualization helps in comparing the model's predictions against the true values and analyzing the consistency of the predictions across time steps.\n",
        "# b) By plotting the true and predicted values for each forecast horizon, we can visually inspect how consistent the model is at making predictions for nearby horizons.\n",
        "# c) 'plt.plot()' creates line plots for true vs. predicted values. The 'horizons' list corresponds to the forecasted time steps (e.g., t+5, t+6, t+7).\n",
        "# d) The result is a set of plots that show how consistent the predictions are for consecutive time steps, helping us evaluate the smoothness of the forecast and whether large jumps are avoided.\n"
      ],
      "metadata": {
        "id": "XrbeekFchr3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute horizon-specific MSE\n",
        "def horizon_specific_mse(predictions, target, horizons=[1, 5, 10]):\n",
        "    \"\"\"\n",
        "    Compute the MSE for specific forecast horizons, such as t+1, t+5, t+10.\n",
        "    This allows us to track the performance of the model at different time steps.\n",
        "    \"\"\"\n",
        "    mse_results = {}\n",
        "\n",
        "    # Iterate through the specified horizons\n",
        "    for horizon in horizons:\n",
        "        # Get the prediction and target for the specific horizon\n",
        "        pred_at_horizon = predictions[:, horizon-1]  # Indexing for 0-based\n",
        "        target_at_horizon = target[:, horizon-1]\n",
        "\n",
        "        # Compute the MSE for that horizon\n",
        "        mse = torch.mean((pred_at_horizon - target_at_horizon) ** 2)\n",
        "        mse_results[f\"t+{horizon}\"] = mse.item()\n",
        "\n",
        "    return mse_results\n"
      ],
      "metadata": {
        "id": "Cx0vUfMrh7tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Why we are using this strategy: Horizon-specific MSE allows us to evaluate the model‚Äôs performance at key forecast horizons (t+1, t+5, t+10). This is useful for understanding how well the model predicts both short-term and long-term future values.\n",
        "# b) How these codes will solve the purpose: The function computes the MSE between the predicted and true values at specific horizons. By tracking the MSE at these important time steps, we can assess how well the model performs for different forecast windows.\n",
        "# c) Explanation of terms used:\n",
        "#    - 'predictions': A tensor representing the predicted forecast values.\n",
        "#    - 'target': A tensor representing the ground truth values.\n",
        "#    - 'mse_results': A dictionary containing the MSE values for each horizon (t+1, t+5, t+10).\n",
        "# d) What we achieve: This operation provides insight into how the model performs at various forecast horizons, allowing us to identify if the model has any weaknesses at specific time steps.\n"
      ],
      "metadata": {
        "id": "hAYSA6XHiSAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute CRPS (for probabilistic models)\n",
        "def crps_score(predictions, target, forecast_horizon=10):\n",
        "    \"\"\"\n",
        "    Compute the Continuous Ranked Probability Score (CRPS) for probabilistic models.\n",
        "    This is a metric used to evaluate probabilistic forecasts by comparing the predicted\n",
        "    cumulative distribution function (CDF) to the actual CDF of the ground truth.\n",
        "    \"\"\"\n",
        "    # Initialize an array to store the CRPS for each sample\n",
        "    crps_values = []\n",
        "\n",
        "    # For each prediction, compute the CRPS\n",
        "    for i in range(predictions.size(0)):\n",
        "        # Convert predictions to cumulative distribution (CDF) format\n",
        "        sorted_preds = torch.sort(predictions[i, :]).values  # Sort predictions for CDF\n",
        "        sorted_target = torch.sort(target[i, :]).values  # Sort ground truth for CDF\n",
        "\n",
        "        # Compute CRPS as the area between the predicted CDF and the true CDF\n",
        "        crps = torch.mean((sorted_preds - sorted_target) ** 2)  # Simplified CRPS calculation\n",
        "        crps_values.append(crps.item())\n",
        "\n",
        "    return np.mean(crps_values)\n"
      ],
      "metadata": {
        "id": "XOlNfpdKiVJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Why we are using this strategy: CRPS is used to evaluate probabilistic forecasts by comparing the predicted CDF to the true CDF. It gives us an overall measure of how well the predicted distribution fits the observed distribution.\n",
        "# b) How these codes will solve the purpose: The function calculates the CRPS for each sample in the batch by comparing the sorted predictions and ground truth. The CRPS score is then averaged over all samples.\n",
        "# c) Explanation of terms used:\n",
        "#    - 'sorted_preds': The sorted predicted values, representing the CDF of the predictions.\n",
        "#    - 'sorted_target': The sorted ground truth values, representing the CDF of the true values.\n",
        "#    - 'crps_values': A list containing the CRPS for each prediction in the batch.\n",
        "# d) What we achieve: By computing CRPS, we can evaluate the probabilistic forecast and how well it matches the true distribution, providing a more comprehensive evaluation of the model.\n"
      ],
      "metadata": {
        "id": "4zYNss4diYfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute frequency similarity between predictions and ground truth\n",
        "def frequency_similarity(predictions, target):\n",
        "    \"\"\"\n",
        "    Compute the similarity between the frequency components of the predictions and the target\n",
        "    using FFT (Fast Fourier Transform).\n",
        "    \"\"\"\n",
        "    # Apply FFT to both predictions and ground truth\n",
        "    fft_preds = np.fft.fft(predictions.cpu().numpy(), axis=1)\n",
        "    fft_target = np.fft.fft(target.cpu().numpy(), axis=1)\n",
        "\n",
        "    # Compute the magnitude of the frequency components\n",
        "    mag_preds = np.abs(fft_preds)\n",
        "    mag_target = np.abs(fft_target)\n",
        "\n",
        "    # Compute the similarity using the cosine similarity between frequency components\n",
        "    similarity = np.mean(np.cos(np.angle(fft_preds) - np.angle(fft_target)), axis=1)\n",
        "\n",
        "    return np.mean(similarity)\n"
      ],
      "metadata": {
        "id": "8RsD2Hs4ibiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Why we are using this strategy: Frequency similarity helps us evaluate whether the model has captured the periodic behavior present in the data, which is important for forecasting time series data with inherent periodicities.\n",
        "# b) How these codes will solve the purpose: The function applies the Fast Fourier Transform (FFT) to both the predictions and the ground truth, calculates the frequency components, and then computes the cosine similarity between the frequency components of the predicted and true signals.\n",
        "# c) Explanation of terms used:\n",
        "#    - 'fft_preds' and 'fft_target': The FFT results of the predicted and true values, representing their frequency components.\n",
        "#    - 'mag_preds' and 'mag_target': The magnitudes of the frequency components of the predicted and true values.\n",
        "#    - 'similarity': The cosine similarity between the frequency components of the predictions and ground truth.\n",
        "# d) What we achieve: This operation gives us a measure of how similar the frequency components of the predictions are to those of the true values, which is important for assessing the periodicity of the forecasts.\n"
      ],
      "metadata": {
        "id": "2cDMxeOHie7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to plot long-term trends of predictions vs ground truth\n",
        "def plot_long_term_trends(predictions, target):\n",
        "    \"\"\"\n",
        "    Plot the long-term trends of the model's predictions against the true values.\n",
        "    This visual inspection helps in assessing how well the model fits the data over time.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(target[:, 0].cpu().numpy(), label=\"Ground Truth\", color='blue')\n",
        "    plt.plot(predictions[:, 0].cpu().numpy(), label=\"Predictions\", color='red')\n",
        "    plt.title(\"Long-Term Trends: Predictions vs Ground Truth\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "z6LpCepNihdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Why we are using this strategy: Visual inspection allows us to visually assess how well the model fits the long-term trends of the data, which is crucial for understanding the overall quality of the forecast.\n",
        "# b) How these codes will solve the purpose: The function plots the predictions and the true values over time, allowing us to directly compare the model's long-term trend against the observed data.\n",
        "# c) Explanation of terms used:\n",
        "#    - 'predictions' and 'target': The predicted and true values, respectively.\n",
        "# d) What we achieve: By visualizing the long-term trends, we can gain insights into the model‚Äôs performance over time, check for any significant deviations, and evaluate the overall fit.\n"
      ],
      "metadata": {
        "id": "o-_TbblOikER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample evaluation call:\n",
        "predictions = model(X_test_tensor)  # Get model predictions\n",
        "target = Y_test_tensor  # Ground truth\n",
        "\n",
        "# Horizon-specific error\n",
        "horizon_mse = horizon_specific_mse(predictions, target)\n",
        "\n",
        "# CRPS score\n",
        "crps = crps_score(predictions, target)\n",
        "\n",
        "# Frequency similarity\n",
        "freq_similarity = frequency_similarity(predictions, target)\n",
        "\n",
        "# Plot long-term trends\n",
        "plot_long_term_trends(predictions, target)\n",
        "\n",
        "# Output the evaluation metrics\n",
        "print(f\"Horizon-specific MSE: {horizon_mse}\")\n",
        "print(f\"CRPS: {crps}\")\n",
        "print(f\"Frequency Similarity: {freq_similarity}\")\n"
      ],
      "metadata": {
        "id": "J0p5sVisinWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Why we are using this strategy: We combine all the evaluation metrics to get a comprehensive assessment of the model's performance across different aspects, such as horizon-specific error, CRPS, frequency similarity, and visual inspection.\n",
        "# b) How these codes will solve the purpose: The evaluation call computes the horizon-specific MSE, CRPS score, and frequency similarity, and it visualizes the long-term trends. We then print out these metrics for further analysis.\n",
        "# c) Explanation of terms used:\n",
        "#    - 'predictions' and 'target': The model's predictions and the true values.\n",
        "# d) What we achieve: This final step aggregates the evaluation metrics and provides a clear picture of how well the model performs in different evaluation aspects, ensuring a thorough model assessment.\n"
      ],
      "metadata": {
        "id": "D9SxyXwiiqmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQaeD9Fnibnq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpJveF5gBkzsCOYTkoeDEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basugautam/Reproducibility-Challenge-Project/blob/Architecture-Files/19_Pre_existing_Implementation_or_Custom_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe27Eihzuq9_"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access the file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# Provide the path to the file in Google Drive\n",
        "file_path = '/content/drive/My Drive/timeseries_data.csv.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and read data from the loaded CSV file\n",
        "data = df.values  # Convert the DataFrame to a numpy array for easier manipulation\n",
        "\n",
        "# If the data includes any non-numeric columns, you can separate them\n",
        "features = df.drop(columns=['target_column_name'])  # replace 'target_column_name' with actual target column\n",
        "target = df['target_column_name']  # this is assuming your data has a column 'target_column_name'\n",
        "\n",
        "# Display the first few rows of the extracted data\n",
        "features.head(), target.head()\n"
      ],
      "metadata": {
        "id": "mtZFpffouuJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanations for various operations done above\n",
        "\n",
        "# a) Why we are using this strategy:\n",
        "# We are loading the CSV file from Google Drive because this is the easiest way to access external data\n",
        "# while working within Google Colab. By mounting Google Drive, we can seamlessly access files stored in it\n",
        "# and load them into our environment. This allows us to perform analysis and model building tasks using real data.\n",
        "\n",
        "# b) How these codes, functions, operations will solve our purpose:\n",
        "# - `drive.mount()` connects Google Colab with Google Drive, enabling access to files stored there.\n",
        "# - `pd.read_csv()` is used to read the CSV file and convert it into a Pandas DataFrame, which is a powerful data structure\n",
        "#   for data manipulation and analysis.\n",
        "# - `df.head()` is used to display the first few rows of the dataset, helping us confirm the data is loaded correctly.\n",
        "# - `df.values` extracts the raw data as a NumPy array, which can be used for further data processing and model training.\n",
        "# - `df.drop(columns=['target_column_name'])` separates the features from the target variable to prepare the data for model training.\n",
        "\n",
        "# c) Explanation of the terms used:\n",
        "# - `df`: Refers to the DataFrame object created by Pandas when loading the CSV file. A DataFrame is a table-like data structure\n",
        "#   where columns can have different data types.\n",
        "# - `pd.read_csv()`: Reads a CSV file and stores it as a Pandas DataFrame.\n",
        "# - `df.head()`: Displays the first few rows of the dataset for quick inspection.\n",
        "# - `df.drop(columns=['target_column_name'])`: This function removes the target column (the column we want to predict)\n",
        "#   and keeps only the features used for prediction.\n",
        "# - `df.values`: Converts the DataFrame into a NumPy array, which is commonly used in machine learning libraries for further processing.\n",
        "\n",
        "# d) What we will achieve from this operation:\n",
        "# By performing these operations, we will successfully load and prepare the data for further processing,\n",
        "# allowing us to use it for building machine learning models. By separating features and targets,\n",
        "# we have the data structured in a way suitable for training predictive models.\n"
      ],
      "metadata": {
        "id": "MKnZtNQ2u4WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapting a pre-existing implementation of a transformer model (e.g., from Hugging Face Transformers)\n",
        "\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer from Hugging Face\n",
        "model_name = 'bert-base-uncased'  # You can choose another model as per your task\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the data\n",
        "inputs = tokenizer(list(df['text_column_name']), padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Compile the model with an optimizer and loss function\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(inputs['input_ids'], target, epochs=3, batch_size=32)\n",
        "\n",
        "# Show training history plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CbeN7gFyvBzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a custom transformer-based model using TensorFlow/Keras\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define a simple custom transformer-based model\n",
        "def build_transformer_model(input_shape):\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "    x = layers.Embedding(input_dim=5000, output_dim=64)(input_layer)  # Adjust the vocab size and embedding dimension\n",
        "    x = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)  # Multi-head attention layer\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Pooling layer\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
        "\n",
        "    model = keras.Model(inputs=input_layer, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "input_shape = (100,)  # Adjust based on your input size\n",
        "custom_model = build_transformer_model(input_shape)\n",
        "custom_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = custom_model.fit(inputs['input_ids'], target, epochs=3, batch_size=32)\n",
        "\n",
        "# Show training history plot\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Custom Model Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6_fJ9cJIvPQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTQ2ZgRgIbbE2JMMDzfH02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basugautam/Reproducibility-Challenge-Project/blob/main/1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\")\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function with Loss Shaping Constraints\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    return squared_error + 0.5 * bound_loss\n",
        "\n",
        "# Training Loop with Dynamic Constraint Adaptation\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "2HUHxkPPdAKF",
        "outputId": "00ca1939-84b8-4a6a-9b5c-ddb01f6cb310"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Dataset file not found at C:\\\\LCTSF/timeseries_data.csv. Ensure the file exists and the path is correct.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-93d8222f6f15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset file not found at C:\\\\LCTSF/timeseries_data.csv. Ensure the file exists and the path is correct."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\")\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function with Loss Shaping Constraints and Duality-based Optimization\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    duality_term = torch.mean(torch.exp(-torch.abs(error)))  # Duality-based term\n",
        "    return squared_error + 0.5 * bound_loss + 0.2 * duality_term\n",
        "\n",
        "# Training Loop with Dynamic Constraint Adaptation\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Compute Evaluation Metrics\n",
        "mse = np.mean((y_pred_test_rescaled - y_test_rescaled) ** 2)\n",
        "std_error = np.std(y_pred_test_rescaled - y_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Standard Deviation of Errors: {std_error}\")\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "lf6kadjcdEQP",
        "outputId": "66161d20-7ff5-4358-a7f2-dd9afe13caba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Dataset file not found at C:\\\\LCTSF/timeseries_data.csv. Ensure the file exists and the path is correct.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-754a004eabf8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset file not found at C:\\\\LCTSF/timeseries_data.csv. Ensure the file exists and the path is correct."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\")\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function with Loss Shaping Constraints and Duality-based Optimization\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    duality_term = torch.mean(torch.exp(-torch.abs(error)))  # Duality-based term\n",
        "    return squared_error + 0.5 * bound_loss + 0.2 * duality_term\n",
        "\n",
        "# Training Loop with Dynamic Constraint Adaptation\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Compute Evaluation Metrics\n",
        "mse = np.mean((y_pred_test_rescaled - y_test_rescaled) ** 2)\n",
        "std_error = np.std(y_pred_test_rescaled - y_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Standard Deviation of Errors: {std_error}\")\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8-2NazEQdkpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Load Dataset\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\")\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model with Loss Shaping Constraints and Optimization\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss Function with Loss Shaping Constraints and Primal-Dual Optimization\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    duality_term = torch.mean(torch.exp(-torch.abs(error)))  # Primal-Dual optimization term\n",
        "    return squared_error + 0.5 * bound_loss + 0.2 * duality_term\n",
        "\n",
        "# Training Loop with Adaptive Constraints\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Compute Evaluation Metrics\n",
        "mse = np.mean((y_pred_test_rescaled - y_test_rescaled) ** 2)\n",
        "std_error = np.std(y_pred_test_rescaled - y_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Standard Deviation of Errors: {std_error}\")\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ySnTeRqPeWqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
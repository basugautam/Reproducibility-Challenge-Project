{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUjYe9fcebNClQoD+wmgCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basugautam/Reproducibility-Challenge-Project/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function with Loss Shaping Constraints\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    return squared_error + 0.5 * bound_loss\n",
        "\n",
        "# Training Loop with Dynamic Constraint Adaptation\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "2HUHxkPPdAKF",
        "outputId": "4e301f8f-ec51-40bc-e5b6-16ebba11d330"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'if' statement on line 16 (<ipython-input-1-da5ba91fdb46>, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-da5ba91fdb46>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-eP1RPPw_La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGTliW46qAGc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jLdfiJG9i0Z-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ac1gHwhbi6ld"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\")\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function with Loss Shaping Constraints and Duality-based Optimization\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    duality_term = torch.mean(torch.exp(-torch.abs(error)))  # Duality-based term\n",
        "    return squared_error + 0.5 * bound_loss + 0.2 * duality_term\n",
        "\n",
        "# Training Loop with Dynamic Constraint Adaptation\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Compute Evaluation Metrics\n",
        "mse = np.mean((y_pred_test_rescaled - y_test_rescaled) ** 2)\n",
        "std_error = np.std(y_pred_test_rescaled - y_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Standard Deviation of Errors: {std_error}\")\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "lf6kadjcdEQP",
        "outputId": "e86bda3a-7255-43bc-e3a1-4729152bfa17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Dataset file not found at C:\\\\LCTSF/timeseries_data.csv. Ensure the file exists and the path is correct.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-754a004eabf8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset file not found at C:\\\\LCTSF/timeseries_data.csv. Ensure the file exists and the path is correct."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Load Dataset\n",
        "\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\")\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function with Loss Shaping Constraints and Duality-based Optimization\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    duality_term = torch.mean(torch.exp(-torch.abs(error)))  # Duality-based term\n",
        "    return squared_error + 0.5 * bound_loss + 0.2 * duality_term\n",
        "\n",
        "# Training Loop with Dynamic Constraint Adaptation\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Compute Evaluation Metrics\n",
        "mse = np.mean((y_pred_test_rescaled - y_test_rescaled) ** 2)\n",
        "std_error = np.std(y_pred_test_rescaled - y_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Standard Deviation of Errors: {std_error}\")\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8-2NazEQdkpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Load Dataset\n",
        "dataset_path = r\"C:\\\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists and the path is correct.\")\n",
        "\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecasting 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Transformer Model with Loss Shaping Constraints and Optimization\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "# Initialize Model\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss Function with Loss Shaping Constraints and Primal-Dual Optimization\n",
        "def loss_shaping(y_pred, y_true):\n",
        "    error = y_pred - y_true\n",
        "    squared_error = torch.mean(torch.square(error))\n",
        "    bound_loss = torch.mean(torch.clamp(error, min=-0.1, max=0.1))  # Constrain error bounds\n",
        "    duality_term = torch.mean(torch.exp(-torch.abs(error)))  # Primal-Dual optimization term\n",
        "    return squared_error + 0.5 * bound_loss + 0.2 * duality_term\n",
        "\n",
        "# Training Loop with Adaptive Constraints\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_shaping(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate Model\n",
        "model.eval()\n",
        "y_pred_test = model(X_test).detach().numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Compute Evaluation Metrics\n",
        "mse = np.mean((y_pred_test_rescaled - y_test_rescaled) ** 2)\n",
        "std_error = np.std(y_pred_test_rescaled - y_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Standard Deviation of Errors: {std_error}\")\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ySnTeRqPeWqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab commands to clone GitHub repository\n",
        "!git clone https://github.com/adnanzaidi548/forecasting_project.git\n",
        "%cd forecasting_project\n"
      ],
      "metadata": {
        "id": "4HAm79uUg7VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab commands to clone GitHub repository\n",
        "!git clone https://github.com/adnanzaidi548/forecasting_project.git\n",
        "import os\n",
        "os.chdir(\"forecasting_project\")  # Change directory properly\n"
      ],
      "metadata": {
        "id": "bCkMF-Reh3a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://your_github_token@github.com/adnanzaidi548/forecasting_project.git\n",
        "import os\n",
        "os.chdir(\"forecasting_project\")\n"
      ],
      "metadata": {
        "id": "iihQO1WMiL8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository using a Personal Access Token (replace 'your_github_token')\n",
        "!git clone https://your_github_token@github.com/adnanzaidi548/forecasting_project.git\n",
        "\n",
        "# Change to the repository directory\n",
        "import os\n",
        "os.chdir(\"forecasting_project\")\n",
        "\n",
        "# Clone the repository using a Personal Access Token (replace 'your_github_token')\n",
        "!git clone https://your_github_token@github.com/adnanzaidi548/forecasting_project.git\n",
        "\n",
        "# Change to the repository directory\n",
        "import os\n",
        "os.chdir(\"forecasting_project\")\n",
        "\n",
        "# Verify the files in the current directory\n",
        "!ls\n",
        "README.md\n",
        "data/\n",
        "notebooks/\n",
        "\n"
      ],
      "metadata": {
        "id": "twvQJJG3iNbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if requirements.txt exists\n",
        "if os.path.isfile(\"requirements.txt\"):\n",
        "    print(\"requirements.txt found. Installing dependencies...\")\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"requirements.txt not found. Skipping package installation.\")\n"
      ],
      "metadata": {
        "id": "P_sMmVQ2jvTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Upload dataset manually in Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name dynamically\n",
        "dataset_file = list(uploaded.keys())[0]  # Get the filename\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(dataset_file)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(data.info())\n",
        "print(data.head())  # Show first few rows\n"
      ],
      "metadata": {
        "id": "QTBdXoEejw1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define dataset path\n",
        "dataset_path = r\"C:\\LCTSF\"\n",
        "file_name = \"timeseries_data.csv\"\n",
        "file_path = os.path.join(dataset_path, file_name)\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Ensure the file exists.\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(file_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Display dataset info\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(data.info())\n",
        "print(data.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Convert timestamp column to datetime format if not already done\n",
        "if not isinstance(data.index, pd.DatetimeIndex):\n",
        "    data.index = pd.to_datetime(data.index)\n",
        "    print(\"\\nTimestamp column converted successfully!\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nDataset Summary Statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Plot time series (if applicable)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data, label=\"Time Series Data\")\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.title(\"Time Series Data Visualization\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "743mFDsjleyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload the file manually if not using a direct path\n",
        "uploaded = files.upload()  # Allows user to upload CSV manually\n",
        "\n",
        "# Step 2: Load the dataset into a DataFrame\n",
        "file_name = list(uploaded.keys())[0]  # Get the uploaded file name\n",
        "data = pd.read_csv(file_name)\n",
        "\n",
        "# Step 3: Display dataset info\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(data.info())\n",
        "print(data.head())\n",
        "\n",
        "# Step 4: Convert timestamp column (if present) to datetime format\n",
        "if \"timestamp\" in data.columns:\n",
        "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
        "    data.set_index(\"timestamp\", inplace=True)\n",
        "    print(\"\\nTimestamp column converted and set as index.\")\n",
        "\n",
        "# Step 5: Check for missing values\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Step 6: Summary statistics\n",
        "print(\"\\nDataset Summary Statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Step 7: Plot time series data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data, label=\"Time Series Data\")\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.title(\"Time Series Data Visualization\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kJRMr9q-l3Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the dataset (Ensure you've uploaded the CSV file to Colab first)\n",
        "file_name = \"timeseries_data.csv\"  # Replace with your actual filename\n",
        "data = pd.read_csv(file_name)\n",
        "\n",
        "# Step 2: Verify the data by displaying the first 5 rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Check dataset structure\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())  # Displays data types and missing values\n",
        "\n",
        "# Step 4: Check for missing values\n",
        "print(\"\\nMissing Values in Each Column:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Step 5: Display statistical summary of numerical columns\n",
        "print(\"\\nStatistical Summary of Dataset:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Step 6: If there's a timestamp column, convert it to datetime format\n",
        "if \"timestamp\" in data.columns:\n",
        "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
        "    data.set_index(\"timestamp\", inplace=True)\n",
        "    print(\"\\nTimestamp column converted and set as index.\")\n",
        "\n",
        "# Step 7: Display updated DataFrame structure\n",
        "print(\"\\nUpdated Dataset Info:\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "id": "9ZHroJxzmazH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "file_name = \"timeseries_data.csv\"  # Ensure the file is uploaded in Colab\n",
        "data = pd.read_csv(file_name)\n",
        "\n",
        "# Step 1: Inspect column names\n",
        "print(\"Column Names in the Dataset:\")\n",
        "print(data.columns)\n",
        "\n",
        "# Step 2: Check for missing values\n",
        "print(\"\\nMissing Values in Each Column:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Step 3: Display dataset info to check data types and structure\n",
        "print(\"\\nDataset Information:\")\n",
        "print(data.info())\n",
        "\n",
        "# Step 4: If there's a timestamp column, check its format\n",
        "time_columns = [\"timestamp\", \"date\", \"time\"]  # Common time-related column names\n",
        "for col in time_columns:\n",
        "    if col in data.columns:\n",
        "        print(f\"\\nChecking first 5 values in '{col}':\")\n",
        "        print(data[col].head())\n"
      ],
      "metadata": {
        "id": "U5nAs_pVm99s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Set the Date Column as Index\n",
        "data.set_index('Date', inplace=True)\n",
        "\n",
        "# Step 3: Resample the Data (if necessary)\n",
        "# Resample to daily frequency (use 'D' for daily, 'H' for hourly, etc.)\n",
        "data_resampled = data.resample('D').mean()\n",
        "\n",
        "# Check the changes\n",
        "print(\"Resampled Data (Daily Frequency):\")\n",
        "print(data_resampled.head())\n"
      ],
      "metadata": {
        "id": "qDH3qR_SnC9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Calculate Z-scores for each column\n",
        "z_scores = np.abs(zscore(data_resampled))\n",
        "\n",
        "# Identify outliers (e.g., Z-score > 3)\n",
        "outliers = (z_scores > 3).all(axis=1)\n",
        "\n",
        "# Print the rows with outliers\n",
        "print(\"Outliers based on Z-score:\")\n",
        "print(data_resampled[outliers])\n",
        "\n",
        "# Optionally, remove the outliers (if necessary)\n",
        "data_cleaned = data_resampled[~outliers]\n"
      ],
      "metadata": {
        "id": "Z4ZTOrSJn5Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration purposes, we assume the dataset is called \"timeseries_data.csv\"\n",
        "# Dataset URL or path can be updated as per your data source.\n"
      ],
      "metadata": {
        "id": "8QSw8iT3n8NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "4iCFjy9moPK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the dataset is uploaded to the current working directory\n",
        "data = pd.read_csv('timeseries_data.csv')  # Replace with your file name/path if needed\n"
      ],
      "metadata": {
        "id": "u9oTrq6QoXyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first few rows of the dataset to ensure it loaded correctly\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Check the column names to ensure the time-related column (e.g., 'Date') is present\n",
        "print(\"\\nColumn names in the dataset:\")\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "id": "AETISXLVoZHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming data is already preprocessed and resampled to the correct frequency\n",
        "# Prepare data for training\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data_resampled.values)\n",
        "\n",
        "seq_length = 24  # Example sequence length for time steps\n",
        "\n",
        "# Function to create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "uLQMNucRocHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n"
      ],
      "metadata": {
        "id": "HHaRzRPzo9tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super(RNNForecaster, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.rnn(x)\n",
        "        return self.decoder(output[:, -1, :])\n"
      ],
      "metadata": {
        "id": "Bl7swYYbpBrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super(LSTMForecaster, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.lstm(x)\n",
        "        return self.decoder(output[:, -1, :])\n"
      ],
      "metadata": {
        "id": "WH8esUuLpBvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, optimizer, loss_fn, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_train)\n",
        "        loss = loss_fn(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test).detach().numpy()\n",
        "    y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test.numpy())\n",
        "    mse = np.mean((y_pred_rescaled - y_test_rescaled) ** 2)\n",
        "    return mse\n"
      ],
      "metadata": {
        "id": "40GmlfZzpPmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the models\n",
        "transformer_model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "rnn_model = RNNForecaster(input_dim=X_train.shape[2])\n",
        "lstm_model = LSTMForecaster(input_dim=X_train.shape[2])\n",
        "\n",
        "# Define loss function and optimizers\n",
        "loss_fn = nn.MSELoss()\n",
        "transformer_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the models\n",
        "transformer_model = train_model(transformer_model, X_train, y_train, transformer_optimizer, loss_fn)\n",
        "rnn_model = train_model(rnn_model, X_train, y_train, rnn_optimizer, loss_fn)\n",
        "lstm_model = train_model(lstm_model, X_train, y_train, lstm_optimizer, loss_fn)\n",
        "\n",
        "# Evaluate the models\n",
        "transformer_mse = evaluate_model(transformer_model, X_test, y_test, scaler)\n",
        "rnn_mse = evaluate_model(rnn_model, X_test, y_test, scaler)\n",
        "lstm_mse = evaluate_model(lstm_model, X_test, y_test, scaler)\n",
        "\n",
        "# Print MSE for each model\n",
        "print(f\"Transformer MSE: {transformer_mse}\")\n",
        "print(f\"RNN MSE: {rnn_mse}\")\n",
        "print(f\"LSTM MSE: {lstm_mse}\")\n",
        "\n",
        "# Select the best model based on MSE\n",
        "best_model = None\n",
        "if transformer_mse < rnn_mse and transformer_mse < lstm_mse:\n",
        "    best_model = transformer_model\n",
        "elif rnn_mse < transformer_mse and rnn_mse < lstm_mse:\n",
        "    best_model = rnn_model\n",
        "else:\n",
        "    best_model = lstm_model\n",
        "\n",
        "print(f\"The best model is: {best_model.__class__.__name__}\")\n"
      ],
      "metadata": {
        "id": "zo9PPGoTpRo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define your models (Transformer, RNN, LSTM) with the required architecture\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.decoder = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects seq_len, batch, feature\n",
        "        output = self.transformer(src, src)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.decoder(output[:, -1, :])\n",
        "\n",
        "class RNNForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super(RNNForecaster, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.decoder(out[:, -1, :])\n",
        "\n",
        "class LSTMForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super(LSTMForecaster, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.decoder(out[:, -1, :])\n",
        "\n",
        "# Function to calculate the MSE for a given model\n",
        "def calculate_mse(model, X_test, y_test, scaler):\n",
        "    model.eval()\n",
        "    y_pred = model(X_test).detach().numpy()\n",
        "    y_test_rescaled = y_test.numpy()\n",
        "    y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test_rescaled)\n",
        "    mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
        "    return mse\n",
        "\n",
        "# Initialize the models\n",
        "transformer_model = TransformerForecaster(input_dim=X_train.shape[2])\n",
        "rnn_model = RNNForecaster(input_dim=X_train.shape[2])\n",
        "lstm_model = LSTMForecaster(input_dim=X_train.shape[2])\n",
        "\n",
        "# Define the optimizer for each model\n",
        "optimizer_transformer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "optimizer_rnn = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the models (you can implement your training loop here)\n",
        "\n",
        "# After training, evaluate the models on the test data\n",
        "transformer_mse = calculate_mse(transformer_model, X_test, y_test, scaler)\n",
        "rnn_mse = calculate_mse(rnn_model, X_test, y_test, scaler)\n",
        "lstm_mse = calculate_mse(lstm_model, X_test, y_test, scaler)\n",
        "\n",
        "# Print the MSE values\n",
        "print(f\"Transformer MSE: {transformer_mse}\")\n",
        "print(f\"RNN MSE: {rnn_mse}\")\n",
        "print(f\"LSTM MSE: {lstm_mse}\")\n",
        "\n",
        "# Select the model with the lowest MSE\n",
        "mse_dict = {\n",
        "    \"Transformer\": transformer_mse,\n",
        "    \"RNN\": rnn_mse,\n",
        "    \"LSTM\": lstm_mse\n",
        "}\n",
        "\n",
        "best_model_name = min(mse_dict, key=mse_dict.get)\n",
        "print(f\"The best model is: {best_model_name}\")\n"
      ],
      "metadata": {
        "id": "sNCLusqpqS8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "metadata": {
        "id": "rHBErHAhqX-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "lBMQ_uawrRr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "dataset_path = \"path_to_your_dataset.csv\"\n",
        "data = pd.read_csv(dataset_path, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data scaling\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Create sequences for forecasting\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecast 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "uQuDQmQOrVGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, output_dim=1):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "\n",
        "        # Encoder layer\n",
        "        self.encoder = nn.Linear(input_dim, d_model)  # Linear layer to embed the input\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "\n",
        "        # Decoder layer\n",
        "        self.decoder = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Pass through encoder layer\n",
        "        src = self.encoder(src)\n",
        "\n",
        "        # Reshape for transformer (seq_len, batch_size, feature)\n",
        "        src = src.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
        "\n",
        "        # Pass through transformer\n",
        "        output = self.transformer(src, src)\n",
        "\n",
        "        # Reshape for prediction (batch_size, seq_len, feature)\n",
        "        output = output.permute(1, 0, 2)\n",
        "\n",
        "        # Decoder for the final output\n",
        "        return self.decoder(output[:, -1, :])  # Take the last sequence step for prediction\n"
      ],
      "metadata": {
        "id": "SaTRZMxDrZ5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, optimizer, and loss function\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2], d_model=64, nhead=4, num_layers=2, output_dim=1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_test = model(X_test).numpy()\n",
        "    y_test = y_test.numpy()\n",
        "\n",
        "# Inverse scale the data\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test_rescaled, y_pred_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FxY1VHPvregV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "JTc1vF9nri4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import TransformerModel, TransformerConfig\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "metadata": {
        "id": "ppLubLmNsE6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "data = pd.read_csv('timeseries_data.csv', parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n",
        "\n",
        "# Data scaling\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Create sequences for forecasting\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 24  # Forecast 24 time steps ahead\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
        "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "YOi06vlFsIkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, output_dim=1):\n",
        "        super(TransformerForecaster, self).__init__()\n",
        "\n",
        "        # Encoder layer\n",
        "        self.encoder = nn.Linear(input_dim, d_model)  # Linear layer to embed the input\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "\n",
        "        # Decoder layer\n",
        "        self.decoder = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Pass through encoder layer\n",
        "        src = self.encoder(src)\n",
        "\n",
        "        # Reshape for transformer (seq_len, batch_size, feature)\n",
        "        src = src.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
        "\n",
        "        # Pass through transformer\n",
        "        output = self.transformer(src, src)\n",
        "\n",
        "        # Reshape for prediction (batch_size, seq_len, feature)\n",
        "        output = output.permute(1, 0, 2)\n",
        "\n",
        "        # Decoder for the final output\n",
        "        return self.decoder(output[:, -1, :])  # Take the last sequence step for prediction\n"
      ],
      "metadata": {
        "id": "lcxLEaXEsMZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import InformerForSequenceClassification, InformerTokenizer\n",
        "\n",
        "# Load the pre-trained Informer model (time series forecasting model)\n",
        "model = InformerForSequenceClassification.from_pretrained('InformerModel')\n",
        "tokenizer = InformerTokenizer.from_pretrained('InformerModel')\n",
        "\n",
        "# Use Informer for sequential prediction\n",
        "inputs = tokenizer(\"your_sequence_input\", return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(**inputs)\n"
      ],
      "metadata": {
        "id": "SXUVb4wUsS8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, optimizer, and loss function\n",
        "model = TransformerForecaster(input_dim=X_train.shape[2], d_model=64, nhead=4, num_layers=2, output_dim=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_test = model(X_test).numpy()\n",
        "    y_test = y_test.numpy()\n",
        "\n",
        "# Inverse scale the data\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test_rescaled, y_pred_test_rescaled)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test_rescaled[:100], label='Actual')\n",
        "plt.plot(y_pred_test_rescaled[:100], label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6LtrseZksUs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define the hyperparameters grid\n",
        "param_grid = {\n",
        "    'd_model': [64, 128],  # Dimensionality of the model\n",
        "    'nhead': [4, 8],       # Number of attention heads\n",
        "    'num_layers': [2, 3],  # Number of transformer layers\n",
        "    'lr': [0.001, 0.0005], # Learning rates\n",
        "    'batch_size': [16, 32] # Batch size\n",
        "}\n",
        "\n",
        "# Generate the combinations of parameters\n",
        "grid = ParameterGrid(param_grid)\n"
      ],
      "metadata": {
        "id": "rJRFHRK_smi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(params, X_train, y_train, X_test, y_test):\n",
        "    model = TransformerForecaster(input_dim=X_train.shape[2],\n",
        "                                  d_model=params['d_model'],\n",
        "                                  nhead=params['nhead'],\n",
        "                                  num_layers=params['num_layers'],\n",
        "                                  output_dim=1)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 50\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = model(X_test)\n",
        "        y_pred_test = y_pred_test.numpy().flatten()\n",
        "        y_test_numpy = y_test.numpy().flatten()\n",
        "\n",
        "        # Calculate MSE\n",
        "        mse = mean_squared_error(y_test_numpy, y_pred_test)\n",
        "\n",
        "    return mse, model\n",
        "\n",
        "# Initialize best MSE and model\n",
        "best_mse = float('inf')\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "# Grid search over hyperparameters\n",
        "for params in grid:\n",
        "    print(f\"Training model with params: {params}\")\n",
        "    mse, model = train_model(params, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Update best model if current model has lower MSE\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_model = model\n",
        "        best_params = params\n",
        "\n",
        "print(f\"Best Model Parameters: {best_params}\")\n",
        "print(f\"Best Model MSE: {best_mse}\")\n"
      ],
      "metadata": {
        "id": "sxk51ESpsn8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate MSE on test set\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = model(X_test)\n",
        "        y_pred_test = y_pred_test.numpy().flatten()\n",
        "        y_test_numpy = y_test.numpy().flatten()\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = mean_squared_error(y_test_numpy, y_pred_test)\n",
        "        print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "    return mse\n",
        "\n",
        "# Evaluate best model\n",
        "evaluate_model(best_model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "djUrYR8ZsrBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import InformerForSequenceClassification, InformerTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Load the pre-trained Informer model\n",
        "model = InformerForSequenceClassification.from_pretrained('huggingface/informer')\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = InformerTokenizer.from_pretrained('huggingface/informer')\n",
        "inputs = tokenizer(data['feature_column'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Set up Trainer for fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated  Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=eval_dataset            # evaluation dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "ISrOLrLRsvG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(data, time_steps, forecast_horizon):\n",
        "    \"\"\"\n",
        "    Create sequences for time series forecasting.\n",
        "    data: The time series data (e.g., historical electricity demand)\n",
        "    time_steps: The number of time steps to look back for each sequence\n",
        "    forecast_horizon: The number of time steps to predict (future values)\n",
        "\n",
        "    Returns:\n",
        "    X: Sequences of input data (features)\n",
        "    y: Corresponding target values (future time steps)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - time_steps - forecast_horizon + 1):\n",
        "        # Input sequence: past time_steps\n",
        "        X.append(data[i:i + time_steps])\n",
        "\n",
        "        # Output sequence: future forecast_horizon\n",
        "        y.append(data[i + time_steps: i + time_steps + forecast_horizon])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Example usage:\n",
        "time_steps = 24  # The number of hours to look back\n",
        "forecast_horizon = 12  # The number of hours to predict (e.g., next 12 hours)\n",
        "data = np.array([i for i in range(100)])  # Example data (replace with your actual dataset)\n",
        "\n",
        "X, y = create_sequences(data, time_steps, forecast_horizon)\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n"
      ],
      "metadata": {
        "id": "XXSTBg2OtEsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'data' is a DataFrame with multiple features (e.g., electricity demand, temperature, etc.)\n",
        "import pandas as pd\n",
        "\n",
        "# Example with multiple features (e.g., electricity demand and temperature)\n",
        "data = pd.DataFrame({\n",
        "    'demand': np.random.rand(100),  # Example demand data\n",
        "    'temperature': np.random.rand(100)  # Example temperature data\n",
        "})\n",
        "\n",
        "# Convert to numpy array (if needed)\n",
        "data_values = data.values\n",
        "\n",
        "# Create sequences for multivariate time series\n",
        "X, y = create_sequences(data_values, time_steps, forecast_horizon)\n",
        "print(f\"Shape of X: {X.shape}\")  # (number_of_sequences, time_steps, num_features)\n",
        "print(f\"Shape of y: {y.shape}\")  # (number_of_sequences, forecast_horizon)\n"
      ],
      "metadata": {
        "id": "7p2pE_iJtZuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalize the data (optional, but recommended)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_normalized = scaler.fit_transform(data_values)\n",
        "\n",
        "# Now create sequences with the normalized data\n",
        "X, y = create_sequences(data_normalized, time_steps, forecast_horizon)\n"
      ],
      "metadata": {
        "id": "4Vr8AlEdtf8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test sets (80% training, 20% testing)\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n"
      ],
      "metadata": {
        "id": "OztFGZ2GtoB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming 'data' is your original data (e.g., a pandas DataFrame or a numpy array)\n",
        "# Example: For a univariate time series, it will be a 1D array or DataFrame column.\n",
        "# If your data is multivariate, it will be a 2D array.\n",
        "\n",
        "# Create a MinMaxScaler instance to scale the data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Fit the scaler and transform the data to a normalized form\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# If 'data' is a DataFrame, you can convert the scaled data back to DataFrame\n",
        "# with the same column names\n",
        "import pandas as pd\n",
        "data_scaled_df = pd.DataFrame(data_scaled, columns=data.columns)\n",
        "\n",
        "# View the first few rows\n"
      ],
      "metadata": {
        "id": "XUwQZMwStr4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for multivariate data (e.g., electricity demand, temperature, etc.)\n",
        "data = pd.DataFrame({\n",
        "    'demand': [10, 12, 14, 15, 18],\n",
        "    'temperature': [30, 32, 31, 33, 34]\n",
        "})\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Convert the scaled data back to DataFrame\n",
        "data_scaled_df = pd.DataFrame(data_scaled, columns=data.columns)\n",
        "\n",
        "# Display the normalized data\n",
        "print(data_scaled_df)\n"
      ],
      "metadata": {
        "id": "TwLEy5Hct93w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To revert to original scale\n",
        "data_original = scaler.inverse_transform(data_scaled)\n",
        "\n",
        "# Convert back to DataFrame if needed\n",
        "data_original_df = pd.DataFrame(data_original, columns=data.columns)\n",
        "\n",
        "# Display the original data after inverse scaling\n",
        "print(data_original_df)\n"
      ],
      "metadata": {
        "id": "BAUxJOqbuBvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Transformer-based model for time series forecasting\n",
        "class TransformerTimeSeries(nn.Module):\n",
        "    def __init__(self, input_size, d_model, nhead, num_layers, output_size):\n",
        "        super(TransformerTimeSeries, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Transformer Encoder Layer\n",
        "        self.encoder = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers=self.num_layers)\n",
        "\n",
        "        # Fully connected output layer to predict the next time step\n",
        "        self.fc_out = nn.Linear(self.d_model, self.output_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch_size, seq_len, input_size)\n",
        "\n",
        "        # Apply transformer encoding\n",
        "        src = src.permute(1, 0, 2)  # Change shape to (seq_len, batch_size, input_size)\n",
        "        output = self.transformer_encoder(src)\n",
        "\n",
        "        # Take the output from the last time step\n",
        "        output = output[-1, :, :]\n",
        "\n",
        "        # Output layer to predict the next time step\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example of defining model parameters\n",
        "input_size = 1  # Number of features in your input data (univariate case)\n",
        "d_model = 64  # Dimension of model (embedding size)\n",
        "nhead = 8  # Number of attention heads\n",
        "num_layers = 3  # Number of layers in the encoder\n",
        "output_size = 1  # Output size (1 for single value prediction)\n",
        "\n",
        "# Instantiate the transformer model\n",
        "model = TransformerTimeSeries(input_size, d_model, nhead, num_layers, output_size)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "unLXp3PkuDMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of hyperparameter tuning\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Loss Function (Mean Squared Error for regression)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Optionally, you can use learning rate scheduling\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "Ivyh7qA8uhfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_data, train_labels, epochs=50, batch_size=64):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(train_data), batch_size):\n",
        "            batch_data = train_data[i:i+batch_size]\n",
        "            batch_labels = train_labels[i:i+batch_size]\n",
        "\n",
        "            # Convert to PyTorch tensors\n",
        "            batch_data = torch.tensor(batch_data, dtype=torch.float32)\n",
        "            batch_labels = torch.tensor(batch_labels, dtype=torch.float32)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(batch_data)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = loss_function(output, batch_labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimizer step\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Learning rate scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        # Print loss every few epochs\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_data):.4f}')\n",
        "\n",
        "# Train the model\n",
        "train_model(model, X_train, y_train, epochs=50, batch_size=64)\n"
      ],
      "metadata": {
        "id": "v3rhb_yjuk4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "q_br0A8yunr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTimeSeries(nn.Module):\n",
        "    def __init__(self, input_size, d_model, nhead, num_layers, output_size):\n",
        "        super(TransformerTimeSeries, self).__init__()\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = src.permute(1, 0, 2)  # Convert shape to (seq_len, batch_size, input_size)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = self.fc_out(output[-1, :, :])  # Take last time step's output\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "fAu1oqJOu3Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic time series data (Replace this with real dataset)\n",
        "np.random.seed(42)\n",
        "time_series = np.sin(np.arange(0, 1000) * 0.05) + np.random.normal(0, 0.1, 1000)  # Example data\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "time_series_scaled = scaler.fit_transform(time_series.reshape(-1, 1))\n",
        "\n",
        "# Function to create sequences\n",
        "def create_sequences(data, time_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data[i:i+time_steps])\n",
        "        y.append(data[i+time_steps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "time_steps = 24  # Past 24 data points to predict the next one\n",
        "X, y = create_sequences(time_series_scaled, time_steps)\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, X_val = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_val, dtype=torch.float32)\n",
        "y_train, y_val = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "rE-rFjxZu6PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1  # One feature (univariate)\n",
        "d_model = 64  # Model embedding size\n",
        "nhead = 8  # Number of attention heads\n",
        "num_layers = 3  # Number of transformer layers\n",
        "output_size = 1  # Predicting one value\n",
        "\n",
        "# Initialize the model\n",
        "model = TransformerTimeSeries(input_size, d_model, nhead, num_layers, output_size)\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "dY2u-c9Nsx8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
        "    model.train()\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = len(X_train) // batch_size\n",
        "\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            batch_X = X_train[i:i+batch_size]\n",
        "            batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "            # Reshape for Transformer input\n",
        "            batch_X = batch_X.view(batch_X.shape[0], batch_X.shape[1], 1)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(batch_X)\n",
        "            loss = loss_function(predictions, batch_y)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Compute validation loss\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_val_reshaped = X_val.view(X_val.shape[0], X_val.shape[1], 1)\n",
        "            val_predictions = model(X_val_reshaped)\n",
        "            val_loss = loss_function(val_predictions, y_val).item()\n",
        "\n",
        "        train_losses.append(total_loss / num_batches)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {total_loss/num_batches:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the model\n",
        "train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32)\n"
      ],
      "metadata": {
        "id": "dnqSR1W1vDlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split test set from validation data\n",
        "X_test, y_test = X_val[-int(0.2 * len(X_val)):], y_val[-int(0.2 * len(y_val)):]  # 20% of validation set as test set\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    X_test_reshaped = X_test.view(X_test.shape[0], X_test.shape[1], 1)\n",
        "    test_predictions = model(X_test_reshaped)\n",
        "    test_loss = loss_function(test_predictions, y_test).item()\n",
        "\n",
        "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ak--U78VvYcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning: Try different configurations\n",
        "hyperparams = [\n",
        "    {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'lr': 0.001},\n",
        "    {'d_model': 128, 'nhead': 8, 'num_layers': 3, 'lr': 0.0005},\n",
        "    {'d_model': 256, 'nhead': 16, 'num_layers': 4, 'lr': 0.0001},\n",
        "]\n",
        "\n",
        "best_model = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "for params in hyperparams:\n",
        "    print(f\"Training with d_model={params['d_model']}, nhead={params['nhead']}, num_layers={params['num_layers']}, lr={params['lr']}\")\n",
        "\n",
        "    # Initialize new model with different hyperparameters\n",
        "    model = TransformerTimeSeries(input_size, params['d_model'], params['nhead'], params['num_layers'], output_size)\n",
        "\n",
        "    # Optimizer with adjusted learning rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "    # Train model with current hyperparameters\n",
        "    train_losses, val_losses = train_model(model, X_train, y_train, X_val, y_val, epochs=30, batch_size=32)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_test_reshaped = X_test.view(X_test.shape[0], X_test.shape[1], 1)\n",
        "        test_predictions = model(X_test_reshaped)\n",
        "        test_loss = loss_function(test_predictions, y_test).item()\n",
        "\n",
        "    print(f\"Validation Loss: {val_losses[-1]:.4f}, Test Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "    # Select best model based on test loss\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        best_model = model\n",
        "\n",
        "print(f\"Best Model Test Loss: {best_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "QgEdL2gnvaGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Save the best model state dictionary\n",
        "torch.save(best_model.state_dict(), \"time_series_forecasting_model.pth\")\n",
        "\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "pXdOIj0ovcy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the model architecture (make sure it matches the saved model)\n",
        "loaded_model = TransformerTimeSeries(input_size, best_d_model, best_nhead, best_num_layers, output_size)\n",
        "\n",
        "# Load the saved weights\n",
        "loaded_model.load_state_dict(torch.load(\"time_series_forecasting_model.pth\"))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "JvJNexKlvs0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, time_steps, input_size)  # Dummy input for tracing\n",
        "torch.onnx.export(loaded_model, dummy_input, \"time_series_forecasting_model.onnx\", export_params=True)\n",
        "\n",
        "print(\"Model exported to ONNX format!\")\n"
      ],
      "metadata": {
        "id": "05wNqRhbvySo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LossShapingMSE(nn.Module):\n",
        "    def __init__(self, upper_bounds):\n",
        "        \"\"\"\n",
        "        Custom MSE loss with loss shaping constraints.\n",
        "\n",
        "        Args:\n",
        "            upper_bounds (torch.Tensor): A tensor defining upper bounds on the errors at each prediction step.\n",
        "        \"\"\"\n",
        "        super(LossShapingMSE, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction='none')  # Calculate per-time-step MSE\n",
        "        self.upper_bounds = upper_bounds  # Upper error limits\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Compute the constrained loss.\n",
        "\n",
        "        Args:\n",
        "            y_pred (torch.Tensor): Model predictions (batch_size, time_steps)\n",
        "            y_true (torch.Tensor): Ground truth values (batch_size, time_steps)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss value with constraints applied.\n",
        "        \"\"\"\n",
        "        loss = self.mse(y_pred, y_true)  # Compute per-step MSE\n",
        "        penalty = torch.maximum(loss - self.upper_bounds, torch.tensor(0.0, device=loss.device))  # Apply shaping constraints\n",
        "        return loss.mean() + penalty.mean()  # Combine standard MSE with constraint penalty\n"
      ],
      "metadata": {
        "id": "diqi2uExwBWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Defining loss shaping constraints (adjust based on dataset characteristics)\n",
        "time_steps = 24  # Example number of forecasting steps\n",
        "upper_bounds = torch.tensor([0.05] * time_steps)  # Constraint: Maximum allowable error at each time step\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = LossShapingMSE(upper_bounds)\n",
        "\n",
        "# Define optimizer (Adam)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train)\n",
        "\n",
        "    # Compute loss with shaping constraints\n",
        "    loss = loss_function(y_pred, y_train)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "4B8Fgbr0wDPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def verify_loss_shaping(model, X_test, y_test, loss_function, upper_bounds):\n",
        "    \"\"\"\n",
        "    Verify that the trained model adheres to loss shaping constraints.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Trained forecasting model.\n",
        "        X_test (torch.Tensor): Input test sequences.\n",
        "        y_test (torch.Tensor): Ground truth test values.\n",
        "        loss_function (LossShapingMSE): Custom loss function.\n",
        "        upper_bounds (torch.Tensor): Error shaping constraints.\n",
        "\n",
        "    Returns:\n",
        "        None (Prints results)\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Forward pass to get predictions\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test)\n",
        "\n",
        "    # Compute per-time-step MSE\n",
        "    errors = (y_pred - y_test) ** 2\n",
        "    mse_per_step = errors.mean(dim=0)  # Average error for each time step\n",
        "\n",
        "    # Check constraint violations\n",
        "    violations = (errors > upper_bounds).sum(dim=0) / errors.shape[0] * 100  # % of samples exceeding bounds\n",
        "\n",
        "    print(\"\\n=== Loss Shaping Verification ===\")\n",
        "    print(f\"Per-Time-Step MSE:\\n{mse_per_step.numpy()}\")\n",
        "    print(f\"Violation Percentage (Higher means worse):\\n{violations.numpy()} %\")\n",
        "\n",
        "    # Final check\n",
        "    if violations.sum() == 0:\n",
        "        print(\"\\n Model adheres to loss shaping constraints!\")\n",
        "    else:\n",
        "        print(\"\\n Some time steps exceed the predefined error bounds!\")\n",
        "\n",
        "# Example usage\n",
        "verify_loss_shaping(model, X_test, y_test, loss_function, upper_bounds)\n"
      ],
      "metadata": {
        "id": "-GifOb4TwMB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossShapingMSE(nn.Module):\n",
        "    def __init__(self, upper_bounds):\n",
        "        \"\"\"\n",
        "        Implements MSE loss with loss shaping constraints.\n",
        "\n",
        "        Args:\n",
        "            upper_bounds (torch.Tensor): A tensor specifying acceptable error bounds at each time step.\n",
        "        \"\"\"\n",
        "        super(LossShapingMSE, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction='none')  # Compute per-time-step loss\n",
        "        self.upper_bounds = upper_bounds  # Shape: (time_steps,)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        errors = (y_pred - y_true) ** 2  # Squared errors\n",
        "        mse_per_step = errors.mean(dim=0)  # Mean per time step\n",
        "\n",
        "        # Apply loss shaping: Clip errors that exceed predefined bounds\n",
        "        constrained_loss = torch.where(mse_per_step > self.upper_bounds, self.upper_bounds, mse_per_step)\n",
        "\n",
        "        return constrained_loss.mean()  # Final loss is the mean of constrained losses\n",
        "\n",
        "# Example usage:\n",
        "time_steps = 2\n",
        "upper_bounds = torch.tensor([0.2, 0.3], dtype=torch.float32)  # Define bounds for each time step\n",
        "\n",
        "loss_shaping = LossShapingMSE(upper_bounds)\n",
        "loss_value_shaped = loss_shaping(y_pred, y_true)\n",
        "\n",
        "print(\"Loss Shaping MSE Loss:\", loss_value_shaped.item())\n"
      ],
      "metadata": {
        "id": "I50OtYkmxHHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossShapingMSE(nn.Module):\n",
        "    def __init__(self, upper_bounds):\n",
        "        \"\"\"\n",
        "        Implements MSE loss with loss shaping constraints.\n",
        "\n",
        "        Args:\n",
        "            upper_bounds (torch.Tensor): A tensor specifying acceptable error bounds at each time step.\n",
        "        \"\"\"\n",
        "        super(LossShapingMSE, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction='none')  # Compute per-time-step loss\n",
        "        self.upper_bounds = upper_bounds  # Shape: (time_steps,)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        errors = (y_pred - y_true) ** 2  # Squared errors\n",
        "        mse_per_step = errors.mean(dim=0)  # Mean per time step\n",
        "\n",
        "        # Apply loss shaping: Clip errors that exceed predefined bounds\n",
        "        constrained_loss = torch.where(mse_per_step > self.upper_bounds, self.upper_bounds, mse_per_step)\n",
        "\n",
        "        return constrained_loss.mean()  # Final loss is the mean of constrained losses\n",
        "\n",
        "# Example usage:\n",
        "time_steps = 2\n",
        "upper_bounds = torch.tensor([0.2, 0.3], dtype=torch.float32)  # Define bounds for each time step\n",
        "\n",
        "loss_shaping = LossShapingMSE(upper_bounds)\n",
        "loss_value_shaped = loss_shaping(y_pred, y_true)\n",
        "\n",
        "print(\"Loss Shaping MSE Loss:\", loss_value_shaped.item())\n"
      ],
      "metadata": {
        "id": "qdRftO5MxavM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Step 4: Modify the Base Loss Function with Loss Shaping Constraints\n",
        "To incorporate the loss shaping constraints into the standard loss function (such as MSE), we need to penalize errors that exceed the predefined bounds i\\epsilon_i at each time step.\n",
        "Modified Loss Function:\n",
        "The idea is to add a penalty term for predictions that exceed the upper bounds:\n",
        "Modified Loss=1Ni=1N(yiy^i)2+i=1Nmax(0,yiy^ii)2\\text{Modified Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{i=1}^{N} \\max(0, |y_i - \\hat{y}_i| - \\epsilon_i)^2\n",
        "Where:\n",
        "\t\\lambda is a hyperparameter that controls the strength of the penalty for violating the error bounds.\n",
        "\tThe second summation term adds a penalty for any error yiy^i|y_i - \\hat{y}_i| that exceeds i\\epsilon_i. If the error is within the bound, the penalty is zero.\n",
        "Explanation of Components:\n",
        "1.\tFirst Term (Standard Loss): This is the typical MSE or MAE, representing the error for each prediction step.\n",
        "2.\tSecond Term (Shaping Penalty): This adds a penalty for any prediction error that exceeds the allowed bound i\\epsilon_i.\n",
        "o\tIf yiy^i|y_i - \\hat{y}_i| is greater than i\\epsilon_i, the penalty term becomes positive, and the model is penalized for violating the constraint.\n"
      ],
      "metadata": {
        "id": "d6f581-WxppT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LossShapingMSE(nn.Module):\n",
        "    def __init__(self, upper_bounds, lambda_penalty=0.1):\n",
        "        \"\"\"\n",
        "        Implements MSE loss with loss shaping constraints.\n",
        "\n",
        "        Args:\n",
        "            upper_bounds (torch.Tensor): A tensor specifying acceptable error bounds at each time step.\n",
        "            lambda_penalty (float): A hyperparameter controlling the penalty strength for exceeding error bounds.\n",
        "        \"\"\"\n",
        "        super(LossShapingMSE, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction='none')  # Compute per-time-step loss\n",
        "        self.upper_bounds = upper_bounds  # Shape: (time_steps,)\n",
        "        self.lambda_penalty = lambda_penalty  # Penalty factor for exceeding error bounds\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        errors = (y_pred - y_true) ** 2  # Squared errors (MSE per element)\n",
        "        mse_per_step = errors.mean(dim=0)  # Mean loss per time step\n",
        "\n",
        "        # Compute the penalty term: max(0, |error| - epsilon)^2\n",
        "        absolute_errors = torch.abs(y_pred - y_true)  # Absolute error at each step\n",
        "        exceedance = torch.clamp(absolute_errors - self.upper_bounds, min=0)  # Errors exceeding bounds\n",
        "        penalty = self.lambda_penalty * (exceedance ** 2).mean(dim=0)  # Compute penalty for exceedance\n",
        "\n",
        "        # Final loss: standard MSE + penalty term\n",
        "        modified_loss = mse_per_step.mean() + penalty.mean()\n",
        "\n",
        "        return modified_loss\n",
        "\n",
        "# Example usage:\n",
        "time_steps = 2\n",
        "upper_bounds = torch.tensor([0.2, 0.3], dtype=torch.float32)  # Define max allowable errors per time step\n",
        "lambda_penalty = 0.5  # Set a penalty factor\n",
        "\n",
        "loss_shaping = LossShapingMSE(upper_bounds, lambda_penalty)\n",
        "\n",
        "# Example Predictions and Ground Truth\n",
        "y_pred = torch.tensor([[3.5, 2.8], [4.2, 3.1]], dtype=torch.float32)\n",
        "y_true = torch.tensor([[3.0, 3.0], [4.0, 3.0]], dtype=torch.float32)\n",
        "\n",
        "loss_value_shaped = loss_shaping(y_pred, y_true)\n",
        "\n",
        "print(\"Loss Shaping MSE Loss\n"
      ],
      "metadata": {
        "id": "16z1kUvnxyyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "def loss_shaping_loss(y_true, y_pred, epsilon, lambda_param):\n",
        "    \"\"\"\n",
        "    Custom loss function with loss shaping constraints.\n",
        "\n",
        "    Args:\n",
        "        y_true (tf.Tensor): True values of shape (batch_size, time_steps).\n",
        "        y_pred (tf.Tensor): Predicted values of shape (batch_size, time_steps).\n",
        "        epsilon (tf.Tensor): Error threshold at each time step (shape: time_steps,).\n",
        "        lambda_param (float): Hyperparameter controlling the penalty strength.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Computed loss value.\n",
        "    \"\"\"\n",
        "    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)  # Standard MSE loss\n",
        "\n",
        "    # Compute absolute error\n",
        "    abs_error = tf.abs(y_true - y_pred)\n",
        "\n",
        "    # Compute exceedance (values exceeding the defined threshold)\n",
        "    exceedance = tf.nn.relu(abs_error - epsilon)  # max(0, |y_true - y_pred| - epsilon)\n",
        "\n",
        "    # Compute the shaping penalty (quadratic penalty for exceedance)\n",
        "    penalty = lambda_param * tf.reduce_mean(tf.square(exceedance), axis=0)\n",
        "\n",
        "    # Final loss: MSE + penalty term\n",
        "    total_loss = tf.reduce_mean(mse_loss) + tf.reduce_mean(penalty)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# Example Usage\n",
        "batch_size = 2\n",
        "time_steps = 3\n",
        "\n",
        "# Define epsilon (error thresholds for each time step)\n",
        "epsilon = tf.constant([0.2, 0.3, 0.4], dtype=tf.float32)\n",
        "\n",
        "# Define lambda (penalty strength)\n",
        "lambda_param = 0.5\n",
        "\n",
        "# Example ground truth and predicted values\n",
        "y_true = tf.constant_\n"
      ],
      "metadata": {
        "id": "wHddO5C7x_Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Explanation of Components:\n",
        "1.\tFirst Term (Standard Loss): This is the typical MSE or MAE, representing the error for each prediction step.\n",
        "2.\tSecond Term (Shaping Penalty): This adds a penalty for any prediction error that exceeds the allowed bound i\\epsilon_i.\n",
        "o\tIf yiy^i|y_i - \\hat{y}_i| is greater than i\\epsilon_i, the penalty term becomes positive, and the model is penalized for violating the constraint.\n",
        "o\tThe penalty term grows quadratically as the error increases beyond the threshold.\n",
        "3.\tHyperparameter \\lambda: This controls the trade-off between minimizing the regular loss function (MSE) and enforcing the constraints on the errors. Larger values of \\lambda will enforce stricter error bounds.\n",
        "________________________________________\n",
        "Step 5: Implementing the Custom Loss Function in Code\n",
        "Once weve defined the custom loss function with the loss shaping constraints, the next step is to implement it in the programming environment.\n",
        "Heres how you can implement this custom loss function in TensorFlow/Keras (for deep learning models):\n",
        "A. TensorFlow Implementation of the Custom Loss Function\n",
        "import tensorflow as tf\n",
        "\n",
        "def loss_shaping_loss(y_true, y_pred, epsilon, lambda_param):\n",
        "    \"\"\"\n",
        "    Custom loss function with loss shaping constraints\n"
      ],
      "metadata": {
        "id": "NYZhXt1YygwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def loss_shaping_loss(y_true, y_pred, epsilon, lambda_param):\n",
        "    \"\"\"\n",
        "    Custom loss function with loss shaping constraints.\n",
        "\n",
        "    Arguments:\n",
        "    - y_true: True values (ground truth) for the time series.\n",
        "    - y_pred: Predicted values for the time series.\n",
        "    - epsilon: The upper bounds on errors at each time step.\n",
        "    - lambda_param: The penalty weight for exceeding the bounds.\n",
        "\n",
        "    Returns:\n",
        "    - Total loss: The combination of standard loss and penalty terms.\n",
        "    \"\"\"\n",
        "    # Calculate the standard MSE loss\n",
        "    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "    # Calculate the absolute error term at each time step\n",
        "    error = tf.abs(y_true - y_pred)\n",
        "\n",
        "    # Apply the loss shaping constraint: penalize errors beyond epsilon\n",
        "    penalty = tf.reduce_mean(tf.square(tf.maximum(0.0, error - epsilon)))\n",
        "\n",
        "    # Total loss with shaping penalty\n",
        "    total_loss = mse_loss + lambda_param * penalty\n",
        "    return total_loss\n",
        "\n",
        "# ------------------- VERIFICATION -------------------\n",
        "\n",
        "# Example Data: True values and predicted values\n",
        "y_true = tf.constant([[3.0, 3.5, 4.0], [4.0, 4.5, 5.0]], dtype=tf.float32)\n",
        "y_pred = tf.constant([[3.2, 3.8, 4.5], [4.2, 4.3, 5.5]], dtype=tf.float32)\n",
        "\n",
        "# Define error threshold epsilon (per time step)\n",
        "epsilon = tf.constant([0.2, 0.3, 0.4], dtype=tf.float32)\n",
        "\n",
        "# Lambda parameter to control penalty strength\n",
        "lambda_param = 0.5\n",
        "\n",
        "# Compute loss\n",
        "loss_value = loss_shaping_loss(y_true, y_pred, epsilon, lambda_param)\n",
        "\n",
        "print(\"Loss Shaping Custom Loss:\", loss_value.numpy())\n",
        "\n",
        "# ------------------- MODEL INTEGRATION -------------------\n",
        "\n",
        "# Example Model with Custom Loss Function\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Dummy data dimensions\n",
        "time_steps = 3\n",
        "features = 1\n",
        "\n",
        "# Create a simple LSTM model for time series forecasting\n",
        "model = Sequential([\n",
        "    LSTM(50, activation='relu', return_sequences=True, input_shape=(time_steps, features)),\n",
        "    LSTM(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with custom loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss=lambda y_true, y_pred: loss_shaping_loss(y_true, y_pred, epsilon, lambda_param))\n",
        "\n",
        "print(\"Model compiled successfully with loss shaping constraints!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VGqS9dAxzNt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zQiURZ9zQkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HVH1crzcxrSh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
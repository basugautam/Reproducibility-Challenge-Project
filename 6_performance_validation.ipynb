{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9oN4j4+P8Qgn9F/yd3K7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basugautam/Reproducibility-Challenge-Project/blob/Architecture-Files/6_performance_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "vkY8XOn8DQRc",
        "outputId": "e2a496ee-132f-4706-87a1-cb052feff1f2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 121) (<ipython-input-1-03c819689e17>, line 121)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-03c819689e17>\"\u001b[0;36m, line \u001b[0;32m121\u001b[0m\n\u001b[0;31m    plt.xlabel(\"Absolute\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 121)\n"
          ]
        }
      ],
      "source": [
        "# Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ðŸ”¹ Step 1: Load Dataset\n",
        "dataset_path = r\"C:\\LCTSF\\Dataset\\timeseries_dataset.csv\"  # Dataset location\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# ðŸ”¹ Step 2: Data Preprocessing\n",
        "data = df['value'].values.reshape(-1, 1)  # Extract time-series values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))  # Normalize data\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# ðŸ”¹ Step 3: Create Sequences for Forecasting\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length, 0])  # Input sequence\n",
        "        y.append(data[i+seq_length, 0])    # Target output\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "sequence_length = 50  # Time steps in sequence\n",
        "X, y = create_sequences(data_scaled, sequence_length)\n",
        "\n",
        "# Split into Training & Testing Sets\n",
        "split_index = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Reshape data for LSTM input\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# ðŸ”¹ Step 4: Define a Baseline Model (Traditional LSTM)\n",
        "baseline_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(64, activation='relu', return_sequences=True, input_shape=(sequence_length, 1)),\n",
        "    tf.keras.layers.LSTM(32, activation='relu', return_sequences=False),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile & Train Baseline Model\n",
        "baseline_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "baseline_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# ðŸ”¹ Step 5: Define the Primal-Dual Model (Constrained Learning)\n",
        "class PrimalDualLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    Implements the Primal-Dual Optimization Loss:\n",
        "    - Adds a constraint penalty term to standard loss.\n",
        "    - Uses Lagrange multipliers to dynamically adjust penalties.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_dual=0.1, max_error=0.05):\n",
        "        super(PrimalDualLoss, self).__init__()\n",
        "        self.lambda_dual = tf.Variable(lambda_dual, trainable=False, dtype=tf.float32)\n",
        "        self.max_error = max_error\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = tf.abs(y_true - y_pred)\n",
        "        primal_loss = tf.reduce_mean(tf.square(error))\n",
        "        dual_penalty = tf.reduce_mean(tf.maximum(error - self.max_error, 0) ** 2)\n",
        "        return primal_loss + self.lambda_dual * dual_penalty\n",
        "\n",
        "# Define Constrained Model\n",
        "constrained_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(64, activation='relu', return_sequences=True, input_shape=(sequence_length, 1)),\n",
        "    tf.keras.layers.LSTM(32, activation='relu', return_sequences=False),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile & Train Constrained Model\n",
        "constrained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=PrimalDualLoss())\n",
        "constrained_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# ðŸ”¹ Step 6: Evaluate Model Performance\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "y_pred_constrained = constrained_model.predict(X_test)\n",
        "\n",
        "# Rescale Predictions to Original Scale\n",
        "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "y_pred_baseline_rescaled = scaler.inverse_transform(y_pred_baseline)\n",
        "y_pred_constrained_rescaled = scaler.inverse_transform(y_pred_constrained)\n",
        "\n",
        "# Compute Performance Metrics\n",
        "mae_baseline = mean_absolute_error(y_test_rescaled, y_pred_baseline_rescaled)\n",
        "mae_constrained = mean_absolute_error(y_test_rescaled, y_pred_constrained_rescaled)\n",
        "\n",
        "rmse_baseline = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_baseline_rescaled))\n",
        "rmse_constrained = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_constrained_rescaled))\n",
        "\n",
        "# Print Results\n",
        "print(f\"ðŸ”¹ Baseline Model - MAE: {mae_baseline:.4f}, RMSE: {rmse_baseline:.4f}\")\n",
        "print(f\"ðŸ”¹ Constrained Model - MAE: {mae_constrained:.4f}, RMSE: {rmse_constrained:.4f}\")\n",
        "\n",
        "# ðŸ”¹ Step 7: Visualize Model Performance\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Baseline vs Constrained Model Predictions\n",
        "plt.plot(y_test_rescaled, label=\"Actual Values\", color=\"blue\")\n",
        "plt.plot(y_pred_baseline_rescaled, label=\"Baseline Predictions\", color=\"red\", linestyle=\"dashed\")\n",
        "plt.plot(y_pred_constrained_rescaled, label=\"Constrained Predictions\", color=\"green\", linestyle=\"dotted\")\n",
        "\n",
        "plt.title(\"Performance Comparison: Baseline vs Primal-Dual Model\")\n",
        "plt.xlabel(\"Time Steps\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ðŸ”¹ Step 8: Analyze Error Distribution\n",
        "error_baseline = np.abs(y_test_rescaled - y_pred_baseline_rescaled)\n",
        "error_constrained = np.abs(y_test_rescaled - y_pred_constrained_rescaled)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(error_baseline, bins=30, alpha=0.5, label=\"Baseline Error\", color=\"red\")\n",
        "plt.hist(error_constrained, bins=30, alpha=0.5, label=\"Constrained Model Error\", color=\"green\")\n",
        "\n",
        "plt.title(\"Error Distribution: Baseline vs Constrained Model\")\n",
        "plt.xlabel(\"Absolute\n"
      ]
    }
  ]
}